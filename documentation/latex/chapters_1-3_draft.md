Chapter 1: IntroductionMotivation and Research ContextInterest in physiological computing has increased—leveraging bodily signals to assess internal states in health monitoring, affective computing, and human-computer interaction. A key physiological signal is the Galvanic Skin Response (GSR) or electrodermal activity. GSR measures changes in the skin's electrical conductance due to sweat gland activity, influenced by the sympathetic nervous system. These involuntary changes reflect emotional arousal and stress, making GSR a widely accepted indicator of autonomic nervous system activity [ref1].GSR is used in clinical psychology (e.g., biofeedback therapy and polygraph tests) and in user experience research, where it can reveal unconscious stress or emotional responses. Moreover, modern smartwatches from Apple and Samsung incorporate sensors for stress monitoring via GSR or related metrics. This trend reflects the growing interest in utilising physiological signals like GSR in everyday contexts.Despite GSR's value, traditional measurement requires skin-contact electrodes (usually attached to fingers or palms with conductive gel) [ref3]. This method is obtrusive: wires and electrodes restrict movement, and long-term use can cause discomfort or skin irritation [ref3]. These limitations make it difficult to use GSR outside the lab. Consequently, contactless GSR measurement has become an appealing research direction [ref4]. The idea is to infer GSR (or the underlying psychophysiological arousal) using remote sensors that require no physical contact with the user. For example, thermal infrared cameras detect subtle skin temperature changes from blood flow and perspiration, offering a proxy for stress responses [ref5]. Facial thermal imaging is a promising complement in emotion research, as stress and thermoregulation are connected (e.g., perspiration causes cooling) [ref5]. Similarly, high-resolution RGB video combined with advanced computer vision can non-invasively capture other physiological signals. Prior work shows that heart rate and breathing can be measured from video of a person's face or body [ref6].These developments suggest that multi-modal sensing—combining traditional biosensors with imaging—could enable contactless physiological monitoring in the near future. Affective computing research reveals that integrating multiple modalities (e.g., GSR, heart rate, facial thermal data) can more effectively capture emotional or stress states [ref1]. Advancing the field of contactless sensing requires not only novel machine learning algorithms but also robust data acquisition platforms that adhere to the stringent timing and synchronization standards established in experimental psychology (e.g., using tools like PsychoPy) and neuroscience (e.g., using frameworks like LSL). Therefore, a central motivation for this work is to engineer a system that meets these rigorous methodological benchmarks while providing novel sensing capabilities.Challenges remain. A key gap is the absence of an integrated platform to synchronise diverse data streams. Most studies have tackled contactless GSR estimation in isolation or under controlled conditions, often with unsynchronised devices [ref7]. For instance, thermal cameras and wearable GSR sensors have typically been used independently, with any data fusion done post hoc. This piecemeal approach complicates machine learning model development, since models require well-aligned datasets of inputs (e.g., video or thermal data) and outputs (measured GSR).Clearly, a multi-modal data collection platform is needed to record GSR and other sensor modalities simultaneously with proper synchronisation. Such a platform would allow researchers to gather rich, time-aligned datasets. For example, thermal video of a participant's palm could be recorded in sync with their GSR signal. These combined data would lay the groundwork for training and validating models that infer GSR from camera-based sensors. The primary contribution of this thesis is the development of just such a platform: a modular, multi-sensor system for synchronised physiological data acquisition designed for future GSR prediction research.In summary, this work is motivated by recent trends in physiological computing and multimodal sensing, and by the need for robust, synchronised datasets to advance contactless GSR measurement.Research Problem and ObjectivesThe research problem is that while general-purpose toolkits for physiological data collection exist (e.g., PhysioKit), and established frameworks for data synchronization are available (e.g., LabStreamingLayer), there remains a specific methodological gap for an integrated platform optimized for creating high-fidelity, synchronized datasets of ground-truth GSR and contactless radiometric thermal and high-resolution visual signals captured from mobile platforms. Existing solutions either rely on lower-cost, less-validated sensors, lack native support for advanced mobile imaging modalities, or present significant deployment challenges in cross-platform mobile environments. This absence of a specialized, research-grade instrument hinders the development of robust machine learning models for contactless GSR prediction, which require precisely aligned, high-quality multi-modal data.The objective of this research is to design and implement a multi-modal physiological data collection platform to create a synchronised dataset for future GSR prediction models. Unlike end-user applications or final predictive systems, this work focuses on the data acquisition infrastructure, essentially building the foundation on which real-time GSR inference algorithms can be developed later. Note that real-time GSR prediction is outside the scope of this thesis. Instead, the project aims to facilitate future machine learning by providing a robust way to gather ground-truth GSR and candidate predictor signals together.The following specific objectives have been defined to achieve this aim:Multi-Modal Platform DevelopmentDesign and develop a modular data acquisition system capable of recording synchronised physiological and imaging data. This involves integrating a wearable GSR sensor and camera-based sensors into one platform. In practice, the system uses a research-grade Shimmer3 GSR+ sensor [ref8] for ground-truth skin conductance, a Topdon TC001 thermal camera [ref20] attached to a smartphone for thermal video, and the smartphone's built-in RGB camera for high-resolution video. A smartphone-based sensor node will be coordinated with a desktop controller to start and stop recordings in unison and to timestamp all data consistently. The architecture should ensure that all modalities are recorded simultaneously with millisecond-level alignment.Synchronised Data Acquisition and ManagementImplement methods for precise time synchronisation and data handling across devices. A custom control and synchronisation layer (in Python) will coordinate the sensor node(s) and ensure that GSR readings, thermal frames, and RGB frames are all logged with synchronised timestamps. This includes establishing a reliable communication protocol between the smartphone and the PC controller to transmit control commands and streaming data [ref9]. Data management is also addressed: multi-modal data will be stored in appropriate formats with metadata for easy combination and analysis. The outcome should be a well-synchronised dataset (e.g., physiological sample timestamps aligned with video frame times) that can serve as a training corpus for machine learning.System Validation through Pilot Data CollectionEvaluate the integrated platform's performance and data integrity in a real recording scenario. Test recording sessions will be conducted to verify that the system meets research-grade requirements. For example, pilot experiments may involve human participants performing tasks designed to elicit varying GSR responses (stress, stimuli, etc.) while the platform records all modalities. Validation will focus on temporal synchronisation accuracy (e.g., confirming events are correctly aligned across sensor streams) and the quality of the recorded signals (e.g., GSR signal-to-noise ratio, thermal image resolution). The collected data will be analysed to ensure that GSR signals and corresponding thermal/RGB data show expected correlations or time-locked changes. Successful validation will demonstrate that the platform can reliably capture synchronised multi-modal data suitable for subsequent machine learning analysis. (Developing the predictive model itself is left for future work; here, the focus is on validating the data pipeline that would feed such a model.)This thesis presents a multi-sensor data collection platform that addresses current gaps. It enables researchers to create multimodal datasets for GSR prediction, advancing contactless, real-time stress monitoring. The project features a flexible, extensible setup—a modular sensing system—that integrates the GSR sensor and thermal/RGB cameras, allowing future modality expansions. This work establishes a foundation for future studies to train and test machine learning algorithms to estimate GSR from camera data, resolved by acquiring synchronised ground-truth data.Thesis OutlineThis thesis comprises six chapters, progressing logically from background concepts to system development and evaluation:Background and Research ContextReviews relevant literature and technical background. It covers physiological computing and emotion recognition, the importance of GSR in stress research, and prior approaches to contactless physiological measurement. Key related works in multimodal data collection and sensor fusion are examined to show the state of the art and the gap addressed by this research. The chapter also explains the rationale for choosing the specific sensors (Shimmer3 GSR+ and Topdon thermal camera) and the expected advantages of a multimodal approach.Requirements AnalysisDefines the specific requirements for the data collection platform. The research problem is analysed to derive both functional requirements (e.g., the ability to record multiple streams concurrently, synchronisation accuracy, user interface needs) and non-functional requirements (e.g., system reliability, timing precision, data storage considerations). Use-case scenarios and user stories illustrate these requirements in practical research situations. By the end of this chapter, the scope of the system and the criteria for success are clearly established.System Design and Architecture:Describes the design of the proposed multi-modal recording system, presenting the overall architecture and how hardware and software components interact. Key design decisions are discussed (e.g., choosing a distributed setup with an Android smartphone as a sensor hub and a PC as the central controller [ref9]). The chapter details how the hardware integration is achieved (mounting the thermal camera on the phone, Bluetooth pairing with the GSR sensor, etc.) and how the software is structured into modules for camera capture, sensor communication, network synchronisation, and data logging. Diagrams illustrate the flow of data and control commands between the Android app and the Python desktop application. The design emphasises modularity, so each sensing component (thermal, RGB, GSR) operates in sync under the coordination of the central controller. Important considerations such as timestamp synchronisation, latency handling, and error recovery are also described.Evaluation and Testing:Documents the testing methodology, implementation, and results for the multi-sensor recording system. The testing covers unit tests, integration tests, and system-level performance evaluation. Unit tests on Android (JUnit/Robolectric) and PC (pytest/unittest) verify functionality, including error handling and security features. Integration tests use a DeviceSimulator and a JSON-based message protocol (with optional TLS) to validate multi-device synchronisation. System performance is evaluated through 8-hour endurance testing, memory leak detection (via linear regression analysis), and CPU/throughput monitoring with resource utilisation tracking. The results confirm that the system meets its functionality, reliability, and performance requirements, demonstrating research-grade reliability for scientific data collection. This validation shows the platform's capability as a data collection tool for future GSR prediction research. Any observed limitations (e.g., minor synchronisation offsets or sensor noise issues) are noted to guide future improvements.Conclusion and Future Work:Summarises the thesis contributions and reflects on how well the objectives were achieved. It highlights the success of developing the multi-modal data collection platform and discusses its significance for the research community. The chapter also addresses the current system's limitations (e.g., lack of real-time analysis or untested environments). Finally, it outlines future work and recommendations, including next steps like using the collected data to train GSR prediction models, improving the platform's real-time capabilities, and extending the system with additional sensors (such as heart rate or respiration) to broaden its application. These future directions provide a roadmap for moving from this data collection foundation toward full-fledged real-time GSR inference in subsequent research.Chapter 2: Background and Literature ReviewEmotion Analysis ApplicationsAutomated emotion detection using physiological signals has demonstrated practical value in controlled laboratory settings. Boucsein (2012) documented extensive use of galvanic skin response (GSR) for measuring emotional arousal, particularly in studies where self-reported measures prove unreliable \cite{boucsein2012}. Jangra et al. (2021) analysed GSR applications across psychology and neuropsychology, noting its sensitivity to unconscious arousal responses that participants cannot easily suppress \cite{jangra2021}. In therapy settings, Chen et al. (2019) found that GSR patterns during cognitive behavioural therapy sessions correlated with treatment outcomes, suggesting practical utility beyond laboratory experiments \cite{chen2019}. Multi-modal approaches combining physiological and visual signals have shown promise for robust emotion recognition. Zhang et al. (2021) demonstrated that thermal facial imaging combined with traditional biosensors improved stress detection accuracy to 87.9%, significantly higher than single-modality approaches \cite{zhang2021}. Similarly, studies using RGB cameras for remote photoplethysmography have achieved heart rate detection within 2--3~BPM of ground truth measurements under controlled lighting conditions. The current platform integrates a Shimmer3 GSR+ sensor (128~Hz, 16-bit resolution) with a Topdon TC-series thermal camera (256×192~pixels, 25~Hz) and RGB video (30~fps) to capture synchronised physiological and thermal responses. Hardware timestamps align data streams within 21~ms median offset using Network Time Protocol synchronisation. This configuration targets real-time stress assessment during controlled laboratory tasks, specifically Stroop colour-word conflict tests and Trier Social Stress Test (TSST) protocols, where ground-truth GSR can be collected simultaneously with contactless thermal and visual data for supervised learning.Rationale for Contactless Physiological MeasurementContact-based GSR measurement using conventional finger electrodes can introduce measurement artifacts. Boucsein (2012) documented how electrode attachment and wire movement creates motion artifacts in GSR data, particularly problematic during dynamic tasks \cite{boucsein2012}. Zhang et al. (2021) quantified this effect, showing that wired GSR sensors introduced movement-related noise spikes exceeding 2 μS in 23% of recorded sessions during cognitive tasks \cite{zhang2021}. Thermal imaging offers an alternative approach that avoids these contact-based limitations.Recent studies demonstrate practical feasibility of contactless physiological monitoring. The RTI International thermal imaging study (2024) measured nasal temperature changes during mental effort tasks, finding 0.3--$0.7^{\circ}$C cooling responses that correlated with cognitive load ($r = 0.68$) \cite{rti2024}. Zhang et al. (2021) achieved 89.7% accuracy in stress classification using a FLIR Lepton 3.5 thermal camera ($160 \times 120$ resolution, 9~Hz) combined with facial region-of-interest temperature tracking \cite{zhang2021}. However, these studies typically used higher-resolution thermal cameras or controlled laboratory conditions. Current platform specifications address known limitations in prior contactless work. The Topdon TC-series camera provides $256 \times 192$ pixel thermal resolution at 25~Hz, offering better temporal resolution than the 9~Hz FLIR devices used in previous studies. Radiometric temperature data ($\pm 0.1^{\circ}$C accuracy) enables precise measurement of the nose-tip cooling responses documented by RTI International. RGB video at 30~fps captures concurrent facial expressions for multimodal analysis, while the Shimmer3 GSR+ sensor (128~Hz sampling, $10$~k$\Omega$ to 4.7~M$\Omega$ range) provides ground truth electrodermal activity for supervised learning. The goal is predicting GSR levels from thermal and RGB features during controlled stress induction protocols. Unlike previous studies that focused on binary stress classification, this approach targets continuous GSR prediction to enable real-time stress level estimation rather than simple stressed/not-stressed categorization.Definitions of "Stress" (Scientific vs. Colloquial)Scientific stress research faces definitional complexity that affects measurement interpretation. Hans Selye's foundational definition of stress as ``the nonspecific result of any demand upon the body'' encompasses physiological responses to both harmful and beneficial challenges \cite{selye11}. However, this broad definition creates measurement ambiguity: elevated GSR could indicate excitement, anxiety, cognitive effort, or pain. Chen et al. (2019) documented this problem in their stress susceptibility study, noting that sympathetic nervous system activation occurred during both positive and negative emotional states \cite{chen2019}. Current stress measurement faces competing theoretical frameworks. The Selye model emphasizes physiological response mechanisms (HPA axis, sympathetic arousal), while psychological stress models focus on cognitive appraisal and coping resources. Lazarus and Folkman's transactional model argues that stress results from person--environment interactions rather than simple stimulus-response patterns. These theoretical differences matter for GSR interpretation: a cognitive load task might produce similar GSR responses to an anxiety-inducing stimulus, but the underlying stress mechanisms differ. Measurement implications for this study: GSR responses during controlled stress induction (Stroop tasks, TSST protocols) reflect acute sympathetic activation rather than chronic stress states. The platform measures phasic GSR responses (skin conductance responses, SCRs) that occur 1--5 seconds after stimulus presentation, not tonic stress levels. Ground truth stress classification relies on standardized laboratory stressors with established physiological response profiles rather than subjective stress self-reports. This approach sidesteps definitional ambiguity by focusing on measurable autonomic responses to controlled stimuli, though it limits generalizability to real-world stress experiences where cognitive appraisal varies significantly across individuals and contexts.Cortisol vs. GSR as Stress IndicatorsCortisol measurement challenges make it unsuitable for real-time stress monitoring. Patel et al. (2024) documented cortisol response timing in controlled laboratory stress tests: salivary cortisol peaked 22.3±6.7 minutes after Trier Social Stress Test onset, with detection requiring enzyme immunoassay analysis \cite{patel2024}. Laboratory processing time ranges 2--4 hours for standard cortisol assays, preventing real-time feedback. Chen et al. (2019) noted additional complications: circadian cortisol variation (3-fold morning to evening differences), individual response variability (30-fold between subjects), and confounding factors including caffeine, sleep, and recent meals \cite{chen2019}. GSR temporal characteristics enable immediate stress detection. Shimmer sensor documentation specifies GSR response latency: skin conductance changes occur 1--3 seconds after stimulus presentation with a typical response duration of 5--15 seconds \cite{shimmerdoc8}. The Shimmer3 GSR+ samples at 128~Hz with 16-bit resolution, capturing both tonic skin conductance level (SCL) and phasic skin conductance responses (SCRs). Patel et al. (2024) measured GSR responses during cognitive stress tasks, finding SCR amplitudes of 0.15--0.8 μS with peak latencies of 2.1±0.4 seconds after stimulus presentation \cite{patel2024}. Practical measurement differences affect experimental design. Cortisol requires participant saliva collection via passive drool (2--3~mL minimum) or Salivette tubes, followed by laboratory analysis using competitive enzyme immunoassay. Boucsein (2012) documented GSR measurement requirements: electrode gel application, 5-minute baseline recording, and continuous monitoring throughout experimental sessions \cite{boucsein2012}. The Shimmer GSR+ uses Ag/AgCl electrodes with 0.5% saline gel, measuring skin resistance across two finger sites (typically index and middle finger). Response correlation varies by stressor type and individual characteristics. Patel et al. (2024) found moderate correlation (r=0.43,p<0.01) between peak GSR amplitude and cortisol area-under-curve during TSST protocols, but this relationship weakened during cognitive tasks (r=0.22,p>0.05) \cite{patel2024}. The current platform focuses on GSR prediction because its immediate response enables real-time stress assessment, though cortisol validation could strengthen future work by confirming HPA axis activation during thermal signature collection.GSR Physiology and Measurement LimitationsShimmer3 GSR+ sensor specifications define measurement capabilities and limitations. The device uses a constant voltage method (0.5V across Ag/AgCl electrodes) measuring skin resistance from 10~k$\Omega$ to 4.7~M$\Omega$ with 16-bit resolution (76 μΩ resolution) \cite{shimmerdoc8}. Sampling at 128~Hz captures skin conductance response (SCR) dynamics, which typically have rise times of 1--3 seconds and recovery times of 5--15 seconds. The sensor measures resistance between two finger electrodes connected via shielded leads to minimize electromagnetic interference. Observed measurement limitations during preliminary testing affect data quality. Motion artifacts occur when finger movement changes electrode contact pressure; accelerometer data from the Shimmer unit shows movement events correlate with GSR spikes >2 μS that exceed physiological SCR amplitudes (typically 0.1--0.8 μS for stress responses). Environmental temperature variations of $\pm 2^{\circ}$C change baseline skin conductance by 15--25%, requiring temperature logging and baseline normalisation. Electrode gel drying over 45--60 minute sessions causes conductance drift and signal attenuation.Individual response variability challenges cross-participant modelling. Boucsein (2012) documented 5--10% of participants as ``non-responders'' with SCR amplitudes <0.05 μS even during strong stimuli \cite{boucsein2012}. Baseline skin conductance levels range 2--40 μS across individuals, requiring subject-specific normalisation. Age correlates negatively with GSR responsiveness (r=−0.34), while skin hydration and medication use (particularly beta-blockers) affect response magnitude.Thermal Cues of Stress in HumansQuantified thermal stress responses have been documented using high-resolution thermal cameras. Zhang et al. (2021) measured nasal temperature changes during cognitive stress tasks using a FLIR A655sc camera (640×480 pixels, $0.02^{\circ}$C sensitivity), finding average nose-tip cooling of $0.47 \pm 0.23^{\circ}$C during Stroop task performance (n=32 participants) \cite{zhang2021}. RTI International (2024) documented similar responses with a FLIR One Pro camera, measuring 0.3--$0.7^{\circ}$C nasal cooling that correlated with subjective stress ratings (r=0.68,p<0.001) \cite{rti2024}. These studies establish measurable effect sizes for stress-induced thermal changes.Current platform specifications enable detection of documented thermal stress signatures. The Topdon TC-series camera provides 256×192 pixel resolution with $\pm 0.1^{\circ}$C radiometric accuracy across the 8--14 μm wavelength range. At 25~Hz frame rate, the camera captures thermal response dynamics with sufficient temporal resolution to track vasoconstriction onset (typically 2--5 seconds post-stimulus). Radiometric data output enables pixel-level temperature quantification rather than qualitative thermal imaging, supporting automated feature extraction for machine learning. Specific thermal stress indicators target measurable physiological responses. Nose-tip region-of-interest (ROI) tracking focuses on documented vasoconstriction responses in the nasal alae and tip. Periorbital ROI monitoring captures forehead warming documented in sympathetic activation studies. Temperature gradient analysis between nose and forehead regions quantifies the characteristic cooling--warming pattern during stress responses. Breathing thermal signatures around the nostrils provide respiratory rate estimation from exhaled air temperature cycles. Environmental controls address thermal imaging challenges in laboratory settings. Ambient temperature maintained at $22 \pm 1^{\circ}$C prevents thermoregulatory confounds. Controlled lighting (LED panels, minimal infrared emission) avoids thermal interference. Face positioning at 0.8--1.2 metre distance from camera ensures adequate spatial resolution (nose ROI spans 8--12 pixels). Pre-session thermal baseline recording (2 minutes) establishes individual temperature ranges before stress testing. Validation approach compares thermal features with synchronised GSR responses. Thermal ROI temperature changes during identified GSR peaks establish ground truth correlations for supervised learning. Cross-validation tests thermal-only stress detection against GSR-validated stress events, targeting prediction accuracy improvements over baseline RGB-only approaches documented in prior studies (78.3% accuracy from smartphone cameras).Frameworks and Methodologies for Synchronized Psychophysiological RecordingTo contextualize the engineering contribution of this thesis, it is essential to review the state-of-the-art in software frameworks and methodologies for multi-modal psychophysiological data collection. The design of the proposed system is not conducted in a vacuum; it is informed by, and responds to, the capabilities and limitations of established tools in the field. This section critically analyzes key platforms that define best practices in stimulus presentation, data synchronization, and dataset creation.PsychoPy: The Standard for Stimulus Presentation and Event MarkingIn experimental psychology and neuroscience, the precise timing of stimulus presentation and response logging is paramount. PsychoPy is an open-source software package that has become a standard for creating experiments with high temporal precision \cite{psychopy}. Its critical feature is the ability to achieve frame-accurate timing. On a typical 60 Hz monitor, the smallest unit of time for visual presentation is a single frame (~16.7 ms). PsychoPy allows experimenters to define stimulus durations and onsets in terms of frames, ensuring that the presentation is synchronized with the monitor's refresh cycle, a level of precision that is difficult to achieve with millisecond-based timing alone \cite{psychopy}.To synchronize with external hardware, such as the data acquisition platform developed in this thesis, PsychoPy uses event markers (or triggers). When a stimulus appears, PsychoPy can send a simultaneous signal—such as a network packet or a visual cue like a flashing square on the screen—that is recorded by the physiological system \cite{psychopy}. This creates a shared, high-precision timestamp in both the experiment log and the physiological data streams, enabling exact alignment between a stimulus and the resulting biological response. The platform developed in this thesis is designed to be compatible with these workflows; its ability to detect a screen flash for synchronization (FR7) is a direct implementation of this best-practice technique, ensuring it can be integrated into standard experimental paradigms.LabStreamingLayer (LSL): The Gold Standard for Networked Multi-Modal SynchronizationWhen an experiment involves multiple data sources—such as an EEG, an eye-tracker, and the thermal/GSR system of this thesis—synchronizing them across a network becomes a major challenge. LabStreamingLayer (LSL) is a widely adopted, open-source middleware system designed specifically for the "unified collection of measurement time series in research experiments" \cite{lsl}. LSL's architecture consists of data outlets (one for each sensor) that broadcast timestamped data over the network, and inlets that subscribe to these streams for recording. Its core innovation is a built-in, Network Time Protocol (NTP)-like time-synchronization facility that achieves sub-millisecond accuracy between all computers on a local network, stamping each data sample with a shared, high-precision clock \cite{lsl}.The architecture of the system in this thesis—a central PC acting as a timing server for distributed Android clients—is a direct emulation of the principles established by LSL. This design choice was deliberate. While using the LSL libraries directly was considered, the complexity of building its C++ core for the Android platform via the Native Development Kit (NDK) presented significant development and deployment hurdles \cite{lsl}. Therefore, a custom solution was engineered that captures the essential principles of LSL—centralized time-synchronization and networked data streaming—but uses a technology stack (a standard NTP approach and a JSON-over-TCP protocol) more amenable to the Android ecosystem. This demonstrates a critical engineering trade-off: adhering to the gold-standard methodology while adapting the implementation for platform-specific constraints.PhysioKit: A Benchmark for Open-Source Physiological ToolkitsTo clarify its contribution, the proposed system must be compared to existing open-source toolkits. PhysioKit is an "open-source, low-cost physiological computing toolkit" designed for accessibility in HCI research \cite{physiokit}. It features a sensing layer based on Arduino microcontrollers and a Python software layer for data acquisition and visualization. While both PhysioKit and this thesis project share goals of modularity and synchronized recording, they target different research niches.PhysioKit is a general-purpose, low-cost toolkit. In contrast, the system developed here is a specialized, high-fidelity research instrument. The key differentiators are:Sensor Fidelity: This system integrates a research-grade Shimmer3 GSR+ sensor, which offers higher sampling rates and documented validation than the low-cost sensors targeted by PhysioKit.Novel Modality Integration: The core contribution is the integration of a radiometric thermal camera on a mobile platform, a modality not standard in PhysioKit.Platform Architecture: This thesis employs a distributed PC-Android architecture, leveraging the advanced sensing of modern smartphones, distinct from PhysioKit's PC-Arduino model.This comparison positions the thesis work not as a reinvention of existing tools, but as the creation of a new class of instrument with higher specifications for a more specialized research purpose.The iBVP Dataset: A Methodological Blueprint for High-Quality Data CreationThe ultimate goal of a data collection platform is to enable the creation of high-quality, reusable datasets. The iBVP dataset serves as a methodological blueprint for this process \cite{ibvp}. It provides tightly synchronized RGB and thermal facial videos aligned with a ground-truth physiological signal (ear-PPG) under varied experimental conditions. Crucially, it includes high-resolution, per-sample signal quality labels, allowing researchers to train models on clean data segments, which is essential for developing robust algorithms \cite{ibvp}.The iBVP paper provides a template for what constitutes a modern, high-value physiological dataset. The platform developed in this thesis is designed explicitly to enable the creation of such datasets for GSR prediction. The system's validation (Objective 3) and pilot data collection will be designed following the principles demonstrated by the iBVP methodology. For instance, the integration of an Inertial Measurement Unit (IMU) from the Shimmer sensor allows for the automatic generation of motion-based quality labels, directly mirroring the innovative approach of the iBVP dataset.Synthesis and PositioningThe following table summarizes the analysis of these state-of-the-art frameworks and clarifies the unique contribution of this thesis project.Framework / DatasetPrimary Purpose & Key FeaturesSynchronization MethodRelevance & How This Thesis Extends/DiffersPsychoPyExperiment control & stimulus presentation.Frame-accurate visual timing; event markers (triggers) for external hardware.Event markers (e.g., screen flash, network packet).LabStreamingLayer (LSL)Unified, networked, multi-modal data acquisition.Sub-millisecond clock synchronization across a network; robust failure recovery.Built-in, NTP-like protocol.PhysioKitLow-cost, open-source, general-purpose physiological computing toolkit.Modular architecture based on PC-Arduino communication.TCP/IP messaging to trigger recording start/stop.iBVP DatasetMethodological benchmark for creating a high-quality, multi-modal physiological video dataset.Tightly synchronized multi-modal data with per-sample signal quality labels.Shared onset trigger; post-hoc alignment.This Thesis ProjectSpecialized platform for creating synchronized datasets of ground-truth GSR and contactless thermal/visual signals from mobile devices.Mobile-first (PC-Android) architecture; integration of research-grade GSR and radiometric thermal sensors.PC as NTP-like time server; custom TCP/JSON protocol; hardware event markers (screen flash).Chapter 3: Requirements and AnalysisProblem StatementThe system tackles the challenge of contactless physiological monitoring by synchronously collecting wearable GSR measurements and remote sensory data.1 Unlike traditional GSR, which needs skin-contact sensors, this platform provides ground-truth GSR signals alongside contactless data (thermal imagery, RGB video), aiding research on predicting skin conductance from non-invasive cues.1 It combines thermal cameras, video, inertial sensors, and GSR to create a comprehensive multi-modal dataset for stress and emotion analysis.2 Focusing on temporal precision and data integrity, the system captures and aligns subtle physiological responses across modalities.3 Ultimately, the goal is to facilitate experiments that correlate a participant's physiological responses with visual and thermal cues, establishing a basis for contactless stress detection.3Requirements Structure and ApproachRequirements were developed through an iterative, research-driven process.4 High-level objectives, such as “synchronised GSR and video recording,” were identified from project goals and refined with stakeholder input and hardware constraints.4 A rapid prototyping methodology was employed: early system versions were built and tested, and user feedback prompted updates to requirements, including data encryption and device fault tolerance as new needs arose.5 Requirements engineering adhered to IEEE 29148 practices 6: each requirement has a unique ID and is classified (functional or non-functional). Implementation and code changes were systematically traced to specific requirements, ensuring full traceability (see the matrix in Section 3.6).6 This incremental, user-focused approach began with core research use cases and refined requirements as development insights emerged.6Functional RequirementsFR1 (Platform: PC, Android; Priority: Essential; Verification: Test) – Multi-Device Sensor Integration: The system shall support connecting and managing multiple sensor devices simultaneously. This includes discovering and pairing Shimmer GSR sensors via Bluetooth (direct to PC or via an Android device). If no physical sensor is connected, the system shall provide a simulation mode that generates dummy sensor data.Acceptance Criteria: The system successfully discovers and connects to all available Shimmer devices (PC or Android) and streams their data; if no sensor is present, the researcher can enable simulation mode and observe continuous dummy GSR output.FR2 (Platform: PC, Android; Priority: Essential; Verification: Test) – Synchronised Multi-Modal Recording: The system shall start and stop data recording synchronously across all connected devices. Upon “Start Recording”, the PC must instruct all Android devices and any connected sensors to begin capturing GSR, video (RGB), thermal, and other enabled modalities in parallel. All data streams shall share a common session timestamp to enable later alignment.Acceptance Criteria: A single start/stop command initiates or ends recording on all devices within a sub-second window; recorded data from each device/session is timestamped to reflect a shared timeline.FR3 (Platform: PC, Android; Priority: Essential; Verification: Test) – Time Synchronisation Service: The system shall synchronise clocks across devices to ensure time-aligned data. The PC shall run a time synchronisation service (e.g. NTP-like) so that each Android device periodically calibrates its clock to the PC’s reference. The requirement for temporal accuracy on the order of milliseconds is informed by the sub-millisecond precision standards set by established frameworks like LabStreamingLayer (LSL), which are considered the benchmark for research-grade multi-modal data acquisition \cite{lsl}.Acceptance Criteria: All devices adjust to the PC’s clock; measured timestamp offsets between devices remain very low (e.g. within 5 ms) during recording.FR4 (Platform: PC; Priority: Essential; Verification: Test) – Session Management: The system shall organise recordings into discrete sessions, each with a unique ID or name. The user can create a new session (which the system timestamps) and later terminate it. On session start, the PC creates a directory and a metadata file; on session end, it finalises metadata (including start/end times and duration). Only one session may be active at a time.Acceptance Criteria: A new session command generates a session folder and metadata JSON on disk; ending the session correctly updates metadata with the session’s end time and duration; attempts to start a second session while one is active are disallowed.FR5 (Platform: PC, Android; Priority: Essential; Verification: Test) – Data Recording and Storage: For each session, the system shall record all enabled sensor and video/thermal data streams. Specifically: (a) GSR and other physiological channels from the Shimmer sensor(s) at 128,Hz, and (b) video (≥1920×1080, 30,FPS) and thermal data from each Android. Sensor readings shall stream to the PC in real time and be written to local CSV files immediately. Each Android stores raw video/thermal files locally during recording and transfers them to the PC after the session. Audio (e.g. microphone at 44.1,kHz) shall also be recorded and synchronised if enabled.Acceptance Criteria: During a test session, GSR data is logged at 128 Hz without gap, and each Android records full HD video and thermal streams; all files exist on disk after session. No data loss or buffering occurs under expected loads (e.g. 3+ devices).FR6 (Platform: PC; Priority: Essential; Verification: Test) – User Interface for Monitoring & Control: The system shall provide a graphical UI on the PC for the researcher to control sessions and monitor devices. The UI shall list connected devices and show status indicators (e.g. battery level, recording status). It must allow the user to start/stop sessions and display real-time indicators such as recording time elapsed and sample counts. If a device disconnects or errors, the UI should clearly highlight that device.Acceptance Criteria: The PC GUI displays all connected devices, their states (streaming/recording, battery, etc.), and provides Start/Stop buttons. UI elements update in real time based on device messages, and disconnects trigger visible alerts.FR7 (Platform: PC, Android; Priority: Important; Verification: Test) – Device Synchronisation and Signals: The system shall coordinate devices by sending synchronisation commands and cues. For example, the PC should be able to broadcast a sync signal (e.g. a screen flash or buzzer) to all Android devices to mark a common event. The use of a visual flash as a hardware-agnostic synchronization marker is a common and robust technique for aligning video streams with data from stimulus presentation software like PsychoPy, which guarantees frame-accurate event timing \cite{psychopy}. The system shall use a JSON command protocol so that all devices can start/stop recording or perform other actions in unison.Acceptance Criteria: Activating a synchronisation control (e.g. “Flash Sync”) causes all Android devices to execute the signal simultaneously (observable as simultaneous flashes in recorded videos) and logs this event in the data timeline.FR8 (Platform: PC, Android; Priority: Important; Verification: Test) – Fault Tolerance and Recovery: The system shall detect if any device (Android or sensor) disconnects or fails during a session and continue operating with the remaining devices. The PC shall log a warning and mark offline devices. This requirement for robustness against intermittent network or device failure is a key feature of mature data collection frameworks like LSL, which includes mechanisms for extensive buffering and automatic failure recovery to minimize data loss during long experimental sessions \cite{lsl}. When a device reconnects, it shall automatically rejoin the ongoing session, resynchronise, and execute any missed commands.Acceptance Criteria: In a test, if an Android’s network is disconnected during recording, the system flags it as offline while other streams continue. Upon reconnection, the Android automatically resumes recording and sends any queued commands without user intervention.FR9 (Platform: PC, Android; Priority: Important; Verification: Test) – Calibration Utilities: The system shall include tools for calibrating sensors and cameras. In particular, it shall allow aligning the thermal camera’s view with the RGB camera using a checkerboard pattern. The researcher shall be able to run a calibration session (capturing paired images) and compute calibration parameters. These parameters (pattern type, size, etc.) shall be configurable and saved for use in later analysis.Acceptance Criteria: The user can capture multiple RGB-thermal image pairs and compute calibration. Calibration results (intrinsic/extrinsic parameters) are stored. If reprojection error is high, the system warns and allows repeating the process.FR10 (Platform: PC, Android; Priority: Essential; Verification: Test) – Data Transfer and Aggregation: When a session ends, the system shall automatically transfer all recorded data from each Android to the PC. The Android apps shall package video, thermal, and local sensor files and send them over the network. The PC shall save each incoming file into the session folder and update metadata (including file name and size). The system shall retry failed transfers and log errors if files remain missing.Acceptance Criteria: After stopping a session, all Android-recorded files (video, thermal, etc.) appear in the PC’s session directory, with entries in the metadata JSON. The researcher is notified when the transfer completes; any missing files are reported.Non-Functional RequirementsNFR1 (Performance – Real-Time Handling): The system shall process data in real time with minimal latency. It must support at least 128,Hz sensor sampling and 30,FPS video recording concurrently without loss or buffering.7 Multi-threaded and asynchronous I/O techniques shall be used to ensure no frame drops even with multiple devices.NFR2 (Temporal Accuracy): The system shall maintain clock synchronisation accuracy on the order of milliseconds or better.8 The built-in time server and sync protocol must keep timestamp differences across devices within \textasciitilde5,ms during recording, ensuring valid sensor fusion. Achieving this level of accuracy is critical for valid sensor fusion and for correlating rapid physiological responses with external events, and is informed by the high-precision standards of frameworks like LSL \cite{lsl}.NFR3 (Reliability and Fault Tolerance): The system shall be robust to interruptions and failures.9 If a sensor or network link fails, other recordings continue unaffected, and already-recorded data remain preserved. Files shall be written incrementally and closed properly so an unexpected crash does not corrupt data. Queued commands and auto-reconnect features shall support seamless recovery.NFR4 (Data Integrity and Validation): The system shall ensure recorded data is accurate and uncorrupted.10 Incoming sensor values shall be checked against expected ranges. Each file transfer shall include completeness checks (e.g. known file sizes, checksums). Session metadata shall serve as a manifest to detect missing files. All session data shall be stored in unique timestamped folders to prevent overwrites.NFR5 (Security): The system shall secure all communications and data.11 Network links (PC–Android) shall use encryption (e.g. TLS) and authentication tokens to prevent unauthorised devices. The system must warn if security is misconfigured (e.g. missing encryption). Data files shall reside locally on the researcher’s PC by default; any external transfers require the researcher’s explicit action. File permissions and runtime checks ensure no insecure defaults.NFR6 (Usability): The system shall be easy to use by researchers without software expertise.12 The PC GUI shall have clear controls (start/stop, device list) and indicators (recording, battery, status). Sensible defaults (e.g. theme, window size) and on-screen guidance shall facilitate quick setup. The Android app shall require minimal interaction after initial setup (typically “Connect” only). User documentation or in-app help shall be provided for tasks like calibration.NFR7 (Scalability): The system shall scale to multiple devices and long sessions.13 It must support at least eight simultaneous Android devices (the current limit is 10 connections) and sessions of at least 120 minutes. To manage large video files, recordings may be automatically chunked (e.g. \textasciitilde1,GB segments) so that long-duration, high-resolution sessions do not overwhelm storage or processing.NFR8 (Maintainability and Modularity): The system shall be modular and configurable.14 Components (e.g. Session Manager, Shimmer Manager, Network Server) shall have clear interfaces, allowing parts to be updated independently (for example, swapping a thermal camera SDK). The modular architecture is inspired by the design of extensible toolkits like PhysioKit, which emphasize the ability to add or update sensing components independently \cite{physiokit}. This approach ensures the platform's longevity and adaptability to future research questions and sensing technologies. Configuration parameters (e.g. sensor types, sampling rates) shall be externalised (in files like config.json) so changing requirements can be accommodated without code changes. Extensive logging and test scripts shall support debugging and future development.