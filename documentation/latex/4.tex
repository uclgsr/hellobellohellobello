\chapter{Design and Implementation}\label{chap:design-implementation}


\section{System Architecture Overview (PC--Android System Design)}\label{sec:4-1}
The system employs a client-server architecture with a central PC coordinating multiple Android capture devices (see ADR-001: Reactive State Management for the rationale). The PC application acts as the master controller, connecting to each Android device over a local network. Each Android device runs a capture app that records sensor data and video, while the PC offers a unified interface to start/stop recordings and aggregate data.

The system architecture (see Figure~\ref{fig:4_01_arch_overview} in Appendix Z) shows how the PC communicates with Android smartphones via Wi-Fi using a custom TCP/IP protocol, sending commands and receiving telemetry (video previews, sensor readings). Android devices autonomously capture data with high-precision clocks for local timestamps, synchronised to the PC's timeline through network time alignment. This design enables multiple phones to record simultaneously within a single session, with the PC serving as the time base. The PC can also integrate local hardware (e.g., a webcam and GSR sensor) alongside the Android data. All captured modalities (video streams, audio, thermal data, GSR signals) are temporally aligned and consolidated on the PC. The result is a distributed recording system where diverse data sources function as a single synchronised unit.


\section{Android Application Design and Sensor Integration}\label{sec:4-2}
The Android application is designed for coordinated multi-modal data capture. At its core is a \texttt{RecordingController} class managing all hardware components and recording tasks. This controller prepares subsystems -- cameras (RGB and thermal), physiological sensors (GSR/PPG), a microphone, etc. When a recording session begins, the controller initialises a new session directory and starts all enabled sensor/camera captures with nanosecond-precision timestamps. Each modality's data is written to device storage in real time. The design utilizes modern Android libraries for performance: CameraX for efficient video and image capture \cite{ref13} and the Nordic BLE library for reliable Bluetooth Low Energy communication with sensors \cite{ref14}. All sensor readings and frames are timestamped with a monotonic clock source to ensure consistency. This modular design (ADR-003: Function Decomposition Strategy) allows easy enabling/disabling of features based on device capabilities (e.g., an inactive thermal camera module if not attached). It also simplifies synchronisation, enabling the controller to manage data sources uniformly (start all, stop all) and trust their timestamps. This modular approach (following ADR-003: Function Decomposition Strategy) facilitates easy feature management based on device capabilities (e.g., inactive thermal camera module). It also streamlines synchronisation logic, enabling uniform handling of data sources (start all, stop all) and reliable timestamping of outputs.

The Android app's internal architecture for thermal camera integration is detailed in Figure~4.2 (see Appendix Z), which provides an overview of the data flow and component interactions.

The following subsections detail the integration of the Topdon thermal camera and Shimmer GSR sensor in the Android app.

\subsection{Thermal Camera Integration (Topdon)}\label{sec:4-2-1}
The Topdon TC001 thermal camera integration on Android has been fully implemented using the production SDK with true hardware interface capabilities. The implementation utilizes the official Topdon SDK classes including \texttt{com.energy.iruvc.ircmd.IRCMD} for device initialization and control, \texttt{com.energy.iruvc.sdkisp.LibIRParse} for raw thermal data parsing, and \texttt{com.energy.iruvc.sdkisp.LibIRProcess} for hardware-calibrated temperature conversion with emissivity correction. The \texttt{ThermalCameraRecorder} class provides production-ready thermal sensing with ±2°C temperature accuracy through hardware-specific calibration and professional thermal imaging features including Iron, Rainbow, and Grayscale color palettes.

The production implementation includes hardware-specific device detection using TC001-specific VID/PID identification (0x0525/0xa4a2, 0x0525/0xa4a5) for reliable device recognition. Temperature processing utilizes the SDK's calibrated conversion algorithms that account for emissivity settings (0.1-1.0 range), ambient temperature compensation, and automatic gain control. The system captures 256×192 thermal frames at 25 Hz with each pixel providing calibrated temperature data rather than generic thermal values.

The thermal data format uses CSV files with headers \texttt{timestamp\_ns,w,h,t0,t1,...,t49151} where each row contains a nanosecond timestamp, frame dimensions (256×192), and 49{,}152 calibrated temperature values in degrees Celsius. A companion \texttt{metadata.json} file stores camera specifications including emissivity settings, calibration parameters, and device-specific thermal correction factors. This production-ready format enables accurate physiological thermal analysis and maintains compatibility with research-grade post-processing tools.

The implementation provides robust error handling with graceful fallback to simulation mode when hardware is unavailable, ensuring development workflow continuity. All SDK method calls include defensive programming with multiple connection attempts and comprehensive resource cleanup through proper SDK disconnect procedures.

\subsection{GSR Sensor Integration (Shimmer)}\label{sec:4-2-2}
The Android app successfully integrates with the Shimmer3 GSR+ sensor through production-ready BLE communication using the official ShimmerAndroidAPI. The \texttt{ShimmerRecorder} class implements complete hardware integration including device discovery, pairing, and streaming data capture with scientifically-accurate 12-bit ADC conversion. The implementation utilizes \texttt{com.shimmerresearch.android.shimmerapi.ShimmerBluetooth} for device communication and \texttt{com.shimmerresearch.android.shimmerapi.ShimmerConfig} for sensor configuration, ensuring production-grade reliability and accuracy.

The production implementation includes robust BLE connection management with automatic device discovery and pairing capabilities. The system properly handles the Shimmer communication protocol by sending \texttt{0x07} commands to initiate streaming and \texttt{0x20} commands to terminate sessions. Most critically, the GSR data processing implements the correct 12-bit ADC conversion (0-4095 range) rather than 16-bit, ensuring scientific accuracy as required for physiological research applications. The \texttt{convertGsrToMicroSiemens()} function provides precise conversion from raw ADC values to microsiemens using the device-specific calibration formulas.

The system captures simultaneous GSR and photoplethysmography (PPG) data at 128 Hz sampling rate with each sample timestamped using the application's \texttt{TimeManager.getCurrentTimestampNanos()} for consistency with other sensor modalities. Data is recorded to CSV files with headers \texttt{timestamp\_ns, gsr\_microsiemens, ppg\_raw}, providing both physiological conductance measurements and cardiovascular signals for comprehensive analysis.

The implementation includes comprehensive error handling and automatic reconnection capabilities for robust operation during extended recording sessions. Device disconnections are detected and handled gracefully with multiple reconnection attempts, ensuring data continuity. The system also provides Lab Streaming Layer (LSL) integration for real-time data monitoring and external system integration while maintaining the primary data recording functionality.

When hardware is unavailable, the system provides intelligent simulation fallback that generates physiologically-realistic GSR patterns for development and testing purposes, ensuring workflow continuity across different environments.

\begin{figure}[htbp]
    \centring
    \includegraphics[width=\textwidth]{../diagrams/fig_4_03_shimmer_gsr_integration.png}
    \caption{Shimmer GSR Integration illustrating the desktop controller application architecture.}
    \label{fig:4_03_shimmer_gsr_integration}
\end{figure}

The Shimmer integration framework provides the structure for future GSR/PPG data streaming at 128 Hz sampling rate with proper conversion from raw ADC values to microsiemens using device-specific calibration formulas. The implementation is designed to support both CSV file storage and LSL stream outputs for live monitoring, with temporal synchronisation maintained through the \texttt{TimeManager.getCurrentTimestampNanos()} method.


\section{Desktop Controller Design and Functionality}\label{sec:4-3}
The desktop controller is a cross-platform application (tested on Windows, Linux, and macOS) built with Qt for the GUI (PyQt6/PySide6) \cite{ref17} and Python 3 for logic, augmented by performance-critical C++ components. The implementation initialises a main window with a tabbed interface. The Dashboard tab provides an overview of connected devices and local sensors, displaying live video feeds and GSR plots in a grid layout. The code sets up a \texttt{QLabel} for video preview and a PyQtGraph \texttt{PlotWidget} for GSR. For example, it can display live video feeds and GSR plots in a grid layout -- the code sets up a \texttt{QLabel} for a video preview and a PyQtGraph \texttt{PlotWidget} for GSR on the dashboard. A Logs tab captures real-time system messages (status updates, errors) for debugging. Another tab for Playback \& Annotation allows reviewing recorded sessions (in a later implementation phase). Each tab's UI elements are created and managed with Qt layouts, making the interface flexible and scalable as devices are added.

The PC controller uses background threads for networking and data processing, preventing UI freezes. A dedicated \texttt{WorkerThread} (a \texttt{QThread} subclass) is responsible for all communication with Android devices. When the user initiates a connection to a device, this worker thread opens a TCP socket to the phone's IP and port. The thread then runs an event loop, receiving JSON messages from the device. When an Android phone sends a preview frame update, the worker decodes the base64 image bytes to a \texttt{QImage} and emits a \texttt{newPreviewFrame} signal with the image and device ID. For instance, if a connected Android phone sends a preview frame update, the worker decodes the base64-encoded image bytes to a \texttt{QImage} and emits a \texttt{newPreviewFrame} signal carrying the image and device identifier. The main GUI thread connects this signal to a slot that displays the frame in the dashboard (e.g., updating the corresponding \texttt{QLabel}'s pixmap). The worker also handles command responses: every command sent to a device includes a unique ID, and the device's reply includes an \texttt{ack\_id} with status info. For example, a \texttt{"capabilities\_data"} response contains the list of cameras the device has -- the worker emits a \texttt{camerasReceived} signal with that list so the UI can populate camera options. This asynchronous message-passing design keeps the GUI responsive and enables the PC to manage multiple devices simultaneously by spawning separate threads for each connection.

The signal-slot mechanism for preview frames is implemented using PyQt's asynchronous messaging system, with details in Appendix F.3 (see Listing F.3 for complete code).

The application implements Zeroconf (mDNS) \cite{ref19} for device discovery: upon startup, the PC browses for \texttt{\_gsr-controller.\_tcp} services on the local network. Each Android device advertises this service type with a name like \texttt{"GSR Android Device [Model]"}. The PC can automatically list available devices and their corresponding addresses when mDNS functions reliably, though manual IP configuration is often required on enterprise networks where mDNS may be filtered or unreliable.

A key feature of the PC controller is its native C++ backend for time-sensitive hardware interaction. This is implemented as a Python extension module (via PyBind11) \cite{ref18} named \texttt{native\_backend}. It provides classes \texttt{NativeWebcam} and \texttt{NativeShimmer} that run in background threads to deliver data to Python with minimal latency. The controller instantiates these at startup: for example, \texttt{NativeWebcam(0)} opens the local webcam (device 0) and begins capturing frames in a loop, and \texttt{NativeShimmer("COM3")} connects to a Shimmer GSR device via a serial port (COM3 on Windows). These native objects start immediately and run independently of the Python GIL, pushing data into thread-safe queues. The GUI uses a \texttt{QTimer} tick (~16 ms) to retrieve the latest data periodically from the native threads. On each tick, it pulls a frame from the webcam class (as a NumPy array) and updates the corresponding video label \cite{ref1}, \cite{ref2}. Similarly, it polls the Shimmer class for new GSR samples and updates the live plot. The native backend captures at a steady ~60 FPS by sleeping ~16 ms per iteration, and yields frames without significant buffering. Meanwhile, the Shimmer thread reads sensor bytes as fast as they arrive (128 Hz) with precise timing. Both use lock-free queues to decouple production and consumption of data. The C++ code directly converts camera frames to a shared memory buffer that is exposed to Python as a NumPy array without copying \cite{ref1}, \cite{ref2}, and similarly packages GSR readings into Python tuples. This design minimises overhead, latency, and jitter -- imperative for synchronising local PC data with remote device data.

Beyond live monitoring, the desktop app includes a framework for post-session analysis. The Playback \& Annotation tab (Figure~4.4) is designed to support loading recorded video files (RGB and thermal) in a synchronised fashion for future implementation. The controller's architecture allows for OpenCV and pandas integration to process data along a unified timeline, with planned features for displaying video frames and corresponding GSR plot points at selected timestamps. The annotation system framework is designed to support user annotations at specific times, which could be saved in sidecar files or session metadata when fully implemented. The PC software also features a Calibration utility for camera calibration post-recording. Using OpenCV \cite{ref22}, it detects calibration patterns (such as chessboards or ChArUco markers) in raw RGB frames to calculate the intrinsic camera parameters. If multiple cameras (e.g., a phone’s RGB and thermal, or a phone and PC webcam) observe the same pattern, it computes their extrinsic calibration. The results, like camera matrices, distortion coefficients, and transformation matrices are saved for data processing. The controller can also export data (often downsampled or compressed) into a single file per session for distribution or analysis. Thus, the desktop controller provides live data acquisition coordination and comprehensive post-processing tools, designed as a modular PyQt6 application with a tab-based architecture.

\begin{figure}[htbp]
    \centring
    \includegraphics[width=\textwidth]{../diagrams/fig_4_04_desktop_gui_layout.png}
    \caption{Desktop GUI Layout illustrating the playback and annotation interface described above.}
    \label{fig:4_04_desktop_gui_layout}
\end{figure}


\section{Communication Protocol and Synchronisation Mechanism}\label{sec:4-4}
A custom communication protocol connects the PC controller with each Android device, built on TCP/IP sockets \cite{ref21} with JSON message payloads. After the PC discovers an Android device (via Zeroconf when available, or manual IP configuration), it initiates a TCP connection to the device's advertised port. The Android app runs a lightweight TCP server to accept this connection. All commands from PC to Android are sent as JSON objects with a schema like \texttt{\{"id": <command\_id>, "command": "<action>", "params": \{ ... \}\}}. The device, upon receiving a command, executes the requested action and then replies with a JSON response containing the original command ID (as \texttt{ack\_id}) and a status or result. For example, the first command the PC sends is \texttt{"query\_capabilities"}, which asks the phone to report its hardware capabilities. The Android app responds with a message like \texttt{\{"ack\_id": 1, "status": "capabilities\_data", "capabilities": \{ ... \}\}} including details such as available cameras (with their identifiers, resolutions, and frame rates). This exchange allows the PC to dynamically adjust to each device -- for instance, listing the camera options or knowing if the thermal sensor is present. Another command is \texttt{"start\_recording"}, which instructs the Android to begin a new recording session. The phone will then initiate all its sensors (cameras, etc.) and reply with an acknowledgement (e.g., \texttt{"status": "ok"}) once recording has successfully started. Similarly, a \texttt{"stop\_recording"} command stops all captures and finalises the files.

The protocol supports continuous data streaming for live previews alongside explicit commands. When idle or recording, the Android app sends \texttt{"preview\_frame"} messages with downsampled camera preview frames encoded as base64 JPEG strings. The PC's worker thread listens for these events and updates the UI, allowing the operator to view a low-latency video feed from each device. This preview is throttled (e.g., one frame every 0.5 seconds) to balance timeliness and network load. Similarly, the app could stream low-rate telemetry (such as current recording status or battery level) using this push mechanism. All asynchronous messages include a \texttt{type} field (e.g., \texttt{"type": "preview\_frame"}) instead of an ack ID, indicating they are unsolicited data, not responses to specific commands.

The communication sequence implements a time synchronisation strategy. Upon connection, the PC and phone perform a handshake, exchanging hello messages and capabilities. Part of this handshake is a time synchronisation routine. The system uses an NTP-inspired algorithm: the PC (acting as a time server) sends a sync request with its timestamp, the phone responds with its timestamp, and the PC measures the round-trip time to estimate network latency. Through one or more exchanges, the PC calculates the offset between its clock and the phone's clock. This offset is then used to relate the timestamps coming from that device. Each device timestamps its data with its local monotonic clock (nanosecond precision), ensuring fine timing granularity. The PC can then translate a device’s timestamps into its master clock domain using the offset. This achieves cross-device synchronisation within sub-millisecond accuracy. In practice, the controller designates its start time as $t = 0$ when recording begins and instructs each Android device to note its local time at that moment. Subsequent data from the phones includes raw timestamps, which are later converted to a common timeline.

To ensure reliability and security, the protocol includes additional features. Each command from the PC expects an acknowledgement; if none is received within a timeout, the PC retries or marks the device as unresponsive. This prevents silent failures (e.g., if a start command is lost, the PC detects and resends it). Communication can optionally be secured with TLS encryption for all messages (commands and data), providing standard transport-layer security when enabled via the \texttt{--enable-tls} configuration flag. Messages are kept compact and human-readable (in JSON format) for debugging and extensibility. For example, a new sensor can be added with a new command and message type without overhauling the protocol, as long as both sides understand the JSON fields.

Synchronisation notably involves the use of the Lab Streaming Layer (LSL). On Android, LSL outlets are created for specific streams (GSR, events, etc.) \cite{ref9}. However, LSL mainly coordinates locally on each device (e.g., marking thermal frame saves relative to GSR samples). Main synchronisation utilises custom network time alignment, under direct application control. By merging precise local timestamps and network clock alignment, the system achieves intra-device sync (e.g., camera vs. All data can be merged on a unified timeline during analysis, needing only microsecond adjustments. All collected data can be merged on a unified timeline during analysis, requiring only microsecond adjustments.

When stopping a recording and collecting files, the protocol ensures a coordinated shutdown. The PC issues \texttt{stop\_recording} to all devices; each device stops and closes its files, then sends back an acknowledgement (or a message like \texttt{"recording\_stopped"} with a summary). The PC can then send a \texttt{"transfer\_files"} command to each device. The Android app compresses its session folder into a ZIP archive (using \texttt{FileTransferManager.transferSession()}) and responds with the file name and size when ready. The file transfer occurs out-of-band to avoid clogging the control channel: the phone opens a new socket to the PC's file receiver on a specified port and streams the file bytes directly. During transfer, the PC may pause other commands or use a separate thread to handle the incoming file. After receiving the file and verifying its checksum, the PC sends a final acknowledgement, and the device may delete its local data. This concludes the session's active phase and transitions to the data processing stage.

\begin{figure}[htbp]
    \centring
    \includegraphics[width=\textwidth]{../diagrams/fig_4_05_protocol_sequence.png}
    \caption{Protocol Sequence illustrating the messaging sequence between PC and Android devices.}
    \label{fig:4_05_protocol_sequence}
\end{figure}

The complete data processing pipeline from capture to export is shown in Figure~4.6 (see Appendix Z). It covers data capture on devices to dataset preparation for analysis. It operates as a streaming pipeline during recording and a batch pipeline after recording. When a new recording session begins on an Android device, a unique session ID is generated based on a timestamp and a directory is created for that session's storage. All data files for the session are saved in this directory, organised by modality. For instance, the app creates sub-folders for raw images and thermal frames in the session directory. This structure ensures that as data streams in, the file system avoids conflicts and simplifies later retrieval.

During an active recording, data from each modality is handled in parallel:
\begin{itemize}
    \item \textbf{RGB Video:} The \texttt{RgbCameraManager} starts recording via CameraX's \texttt{VideoCapture} API to an MP4 file on the device. The file is typically named \texttt{RGB\_\<sessionId\>.mp4} and saved in the session folder. Video is encoded with H.264 at 1080p 30 FPS (\texttt{Quality.HD}) as configured. Recording continues until stopped, at which point the file is finalised (CameraX handles closing the file and muxing the audio track if one was included).
    \item \textbf{Raw Image Stream:} If enabled, the app captures full-resolution still images continuously during the recording. The \texttt{RgbCameraManager} uses an \texttt{ImageCapture} use case to take a picture roughly every 33 ms (~30 FPS) on a background executor. Each image is saved as a JPEG file in a \texttt{raw\_rgb\_\<sessionId\>} directory, with a filename containing its exact nanosecond timestamp (e.g., \texttt{raw\_rgb\_frame\_\<timestamp\>.jpg}). These images are unprocessed (straight from the camera sensor in YUV format converted to JPEG) to allow later analysis or calibration. By capturing them concurrently with video, the system provides both a compressed continuous video and a series of key frames that can be examined frame-by-frame at full quality.
    \item \textbf{Thermal Frames:} The \texttt{TopdonThermalCamera} writes thermal data frames to a CSV file (or sequence of CSVs). In this implementation, it creates one CSV named \texttt{thermal\_data\_\<sessionId\>.csv} in a \texttt{thermal\_\<sessionId\>} directory when streaming starts \cite{ref16}. The first row is a header with pixel index labels, and each subsequent row corresponds to one thermal image frame -- the first column is the frame timestamp and the rest are temperature values \cite{ref16}. (If needed, the system could also save thermal images by converting the temperature matrix to a greyscale or colour-mapped image, but the current design prioritises numerical data for precision.)
    \item \textbf{GSR/PPG Data:} The Shimmer GSR+ sensor data is logged to a CSV file named \texttt{GSR\_\<sessionId\>.csv} in the session folder \cite{ref15}. The file begins with a header (\texttt{timestamp\_ns, GSR\_uS, PPG\_raw}), and each subsequent line represents one sample, as recorded by the \texttt{ShimmerGsrSensor} described earlier \cite{ref15}. Sampling at 128 Hz means this file grows by 128 lines per second of recording. The timestamps are the phone's nanosecond ticks, which will later be re-aligned to the global timeline.
    \item \textbf{Audio:} The app can also record audio via the microphone (stereo 44.1 kHz) if enabled. Audio is captured using Android's MediaRecorder (or AudioRecorder) API and saved as an AAC-encoded track, either in its own file (e.g., \texttt{Audio\_\<sessionId\>.m4a}) or multiplexed into the RGB video MP4. In this system, audio was stored separately -- having a separate audio file with a known start time simplifies synchronisation during analysis.
    \item \textbf{Annotations/Events:} The system is designed to support user markers and automated event logging during recording sessions. The framework includes provisions for a \texttt{SessionManager} to generate session metadata files including start/stop times, device information, and event timestamps. However, comprehensive session metadata JSON generation and annotation features are currently planned for future implementation phases.
\end{itemize}

Once the PC issues a stop command, each Android device closes its files. Next, data aggregation begins. The PC requests each phone to send its session data. To streamline this, the Android app compresses its session folder into a single ZIP archive using a \texttt{FileTransferManager.zipSession()} method. This zips up all files (video, images, CSVs, etc.) from that recording session. The app places this ZIP in a cache directory and then uses \texttt{FileTransferManager.sendFile()} to initiate a transfer to the PC. The transfer occurs via a socket stream \textemdash since the phone knows the PC's IP and designated upload port (provided during the handshake). It opens a connection and streams the ZIP file bytes. On the PC, a file receiver listens and writes the incoming bytes to a file (usually named with the device name and session ID). A progress indicator in the PC UI (e.g., a \texttt{QProgressDialog}) informs the user that data is being downloaded from the device.

After collection, the PC stores all device data. Post-processing begins. The PC controller analyzes session archive data. For instance, the playback module accesses video files and sensor CSVs to replay sessions. With accurate timestamps, aligning streams is straightforward: the GSR plot appears on a time axis (seconds or milliseconds), and video frames align with their timestamps (the controller can use video file timestamps or infer them from image filenames). The annotation tool adds markers to the timeline and links notes to video frames.


For research use cases, exporting data is essential. The pipeline ends with an export step, where the session's raw data is converted to shareable formats. A script or UI action triggers this export, utilising libraries like \texttt{pandas} \cite{ref23} and \texttt{h5py} \cite{ref24} to compile data into HDF5 or MATLAB files. It may create a structured dataset where each sensor modality is a group or table (e.g., an HDF5 group \texttt{/GSR} containing a timestamp array and a GSR value array, and a group \texttt{/Video} containing video frame timestamps or references). If available, calibration results are included to map pixel coordinates in videos to real-world units. The exported files enable researchers to load the entire session in tools like MATLAB or Python with a single command, ensuring all streams are synchronised.

The pipeline synchronises and labels data from capture to archive, ensuring easy access and interpretation during analysis. Automated zipping and transferring eliminate manual steps, and structured session directories prevent mix-ups between sessions or devices.

\begin{figure}[htbp]
    \centring
    \includegraphics[width=\textwidth]{../diagrams/fig_4_06_data_processing_pipeline.png}
    \caption{Data Processing Pipeline providing an overview of the data processing pipeline from capture through to data export.}
    \label{fig:4_06_data_processing_pipeline_repeat}
\end{figure}


\section{Implementation Challenges and Solutions}\label{sec:4-6}

\begin{itemize}
    \item \textbf{Ensuring Precise synchronisation :} Achieving tight time synchronisation across multiple Android devices and the PC was non-trivial due to clock drift and network latency. The solution was a two-tier synchronisation mechanism. Each device timestamps data with a local high-resolution clock (avoiding reliance on internet time or coarse NTP time, which can be imprecise on mobile). Then, a lightweight NTP-like protocol aligns those clocks by calculating the offset and delay. This was fine-tuned by taking multiple measurements at connection time and occasionally during recording. The result is that all devices maintain a shared notion of time within sub-millisecond tolerance. In practice, this means that if two cameras and a GSR sensor capture an event (e.g., an LED flash), the timestamps recorded by each device for that event differ by less than 1 ms after alignment. \emph{Solution highlights:} use monotonic clock APIs on each platform for timestamping and perform a quick clock sync handshake for alignment at start (and periodically if needed).
    \item \textbf{High Data Throughput and Storage Management:} Recording high-definition video alongside high-frequency sensor data can quickly overwhelm device I/O and memory if not handled efficiently. Several strategies were employed. First, writing to storage was done sequentially using buffered streams, which is efficient for both video and CSV writes. Raw image capture posed a challenge because saving a JPEG every 33 ms could saturate the I/O. This was mitigated by performing image captures on a dedicated single-threaded executor separate from the main thread, ensuring the CameraX pipeline had its own thread and disk writes did not block the UI or sensor reads. The system also avoids keeping large amounts of data in memory; for example, thermal frames are written directly to a file within the frame callback, and GSR samples are appended to a file (and optionally to a small in-memory buffer for LSL) one by one. Because Android devices have limited storage, another challenge was preventing extended sessions (which generate many image files and large videos) from filling up the device. The solution was to offload data as soon as possible: immediately after each session, the phone compresses the session folder to a ZIP and transfers it to the PC. The phone can then optionally delete its local copy ("delete after transfer"), so even if multiple sessions are recorded back-to-back, the bulk of data resides on the PC.
    \item \textbf{Thermal Camera USB Integration:} Using the Topdon TC001 thermal camera introduced challenges in driver support and performance. Android does not have native support for USB thermal cameras, so the UVCCamera library was used. However, this required handling USB permissions and ensuring real-time performance in Java. One issue was the volume of data (nearly 50k float values per frame). Pushing this through the Java/Kotlin layer every frame could be slow. The chosen approach was to leverage the library's native (JNI) code to fetch frames and do minimal processing in Kotlin -- essentially just copying the buffer to a file. Writing frames as raw floats to CSV avoided any expensive image rendering computations during capture. Another challenge was potential USB dropouts if the app couldn't keep up with the frame rate. We addressed this by monitoring the frame callback speed; if frames started queuing up, the app would drop some preview processing to catch up, ensuring the logging thread always runs at high priority. Additionally, upon connection, the camera is explicitly set to the correct mode (frame size and format) to avoid negotiation issues. Handling the permission prompt promptly was also important -- the app requests USB permission as soon as the device is detected, so by the time the user starts recording, the camera is already authorised and ready to open.
    \item \textbf{Reliable BLE Sensor Streaming:} The Shimmer GSR+ streaming over BLE can be susceptible to packet loss or disconnects (e.g., in electrically noisy environments or if the phone's BLE stack is busy). This was addressed by utilising Nordic's robust Android BLE library, which offers buffered writes, automatic retries, and straightforward callback management. For example, on connection the code retries the BLE connection up to 3 times with a short delay to overcome transient failures during the handshake. Moreover, once connected, the app immediately sets up notifications and starts streaming to establish a stable data flow. The design of writing data to file versus sending it live was carefully balanced: writing every sample to CSV ensures no data loss (even if the UI or network is slow, the data is safely on disk), while the live LSL broadcast is best-effort (missing a few samples on the live graph is acceptable as long as the file has the full record). It is also essential to send the stop command to the Shimmer device before disconnecting to terminate its stream gracefully; otherwise, if a user restarts recording immediately, the Shimmer might still be in streaming mode and require a reset. By sending \texttt{0x20} (stop) and waiting briefly, the sensor returns to a known idle state. These precautions improved the BLE link reliability, allowing even hour-long recordings to proceed without dropout.
    \item \textbf{Cross-Platform Performance in the PC App:} Python is interpreted and could become a bottleneck for real-time video and sensor handling. Initially, the system used OpenCV \cite{ref22} in Python for webcam capture and PySerial for GSR, but latency and jitter were noticeable (tens of milliseconds of variability). The solution was to implement those parts in C++ and integrate via PyBind11 \cite{ref18}. The \texttt{NativeWebcam} class utilises OpenCV's \cite{ref22} \texttt{VideoCapture} in a separate thread to capture frames and enqueue them. As C++ code, it's compiled and optimised, running independently of the Python GIL. The frame rate became very stable (the thread sleeps for ~16 ms to achieve ~60 FPS, matching the display refresh rate), and frame delivery to Python is achieved by sharing the memory pointer of the \texttt{cv::Mat} with NumPy — essentially, zero-copy image sharing. Similarly, the \texttt{NativeShimmer} class opens a serial port (using Win32 API on Windows or termios on Linux) and reads bytes in a tight loop. It applies the same GSR conversion formula as the Android (a mirror implementation in C++) and pushes timestamped samples into a queue. Measurements showed a ~67\% reduction in end-to-end latency (sensor update to plot update) and ~79\% reduction in timing jitter on the PC side after using the native backend. The trade-off was the added complexity of compiling C++ code for multiple platforms; however, this was mitigated by using CMake and continuous integration testing on each OS.
    \item \textbf{User Interface and Multi-Device Coordination:} Another challenge was designing a GUI that could handle multiple device feeds without overwhelming the user or the system. This was solved with a dynamic grid layout on the Dashboard: as devices connect, new video preview widgets are added (and corresponding plot widgets if the device has a sensor). Qt's layouts automatically manage positioning. Ensuring that updating these widgets (especially painting video frames) happens on the GUI thread was crucial. The solution was to use Qt's signal/slot mechanism -- the background thread emits a signal with a \texttt{QImage}, and the main thread's slot sets that image on a \texttt{QLabel}. This approach is thread-safe and keeps heavy lifting off the UI thread. For many devices streaming simultaneously, simple frame rate limiting on previews was also implemented (each phone sends at most a fixed number of preview frames per second) to prevent flooding the network or GUI. On the PC side, each preview feed utilises a deque buffer, allowing it to drop old frames if the UI is slow to update, rather than accumulating an ever-growing backlog. Additionally, coordinating the start of recording across devices was challenging -- if one device started even 100 ms later than another, that would introduce a sync error. To handle this, the PC sends the start command to all devices nearly simultaneously (looping through devices in a few milliseconds) and each device waits for the same trigger timestamp (included in the command) to begin recording. In effect, the PC says "start recording at time $T = XYZ$," and all devices schedule their start at their local time corresponding to $XYZ$. This was achieved by having devices continuously sync their clocks with the PC during an active session (making slight adjustments) or simply relying on the initial offset if drift is minimal over a short period. The outcome is that all devices begin capturing within a few milliseconds of each other, which the synchronisation logic then corrects to under 1 ms alignment.
\end{itemize}


\section{Ethical Considerations and Data Handling}\label{sec:4-7}

\subsection{Ethics Approval}
Research using this system has been approved by the UCLIC Ethics Committee (Project ID: 1428) for the project titled "Investigating AI and Physiological Computing: App for Camera-based Contactless Sensing of Physiological Signals" led by Prof. Youngjun Cho.

\subsection{Technical Data Protection Implementation}

\textbf{Encryption and Storage}: Android devices utilise \texttt{PrivacyManager.kt} with AES256-GCM encryption via Android Keystore for local storage. Session data is encrypted at rest using the \texttt{EncryptedSharedPreferences} API with master key rotation per recording session. PC-Android communication employs TLS 1.2+ as enforced by \texttt{RuntimeSecurityChecker.py}, validating encryption at startup.

\textbf{Data Anonymisation}: The \texttt{PrivacyManager} class configures participant ID codes (\texttt{participant\_id} field) without storing personal identifiers with sensor data. Video anonymisation applies face detection and blurring when \texttt{face\_blurring\_enabled} is true in the privacy configuration.

\textbf{Access Controls}: The PC controller requires authentication through the \texttt{SecurityUtils} module, with session logs kept by \texttt{SecureLogger.kt} on Android devices. The runtime security checker verifies file access permissions to prevent unauthorised data exposure.

\textbf{Data Retention}: The system has configurable retention policies through the \texttt{data\_retention\_days} parameter (default 365 days) with automated cleanup by session management.

\subsection{Device Safety Specifications}
Hardware safety is ensured through specific configurations:
\begin{itemize}
    \item \textbf{GSR sensors}: Shimmer3 GSR+ devices operate at 3V with current-limited outputs ($<1$ mA) as specified in the device datasheet \cite{ref8}.
    \item \textbf{Thermal imaging}: Topdon TC001 uses passive LWIR sensing (8--14 $\mu$m wavelength) with no energy emission toward participants \cite{ref16}.
    \item \textbf{Wireless protocols}: Bluetooth LE operates at 2.4 GHz within regulatory power limits (Class 2, 2.5 mW), Wi-Fi communication follows 802.11 standards.
\end{itemize}
