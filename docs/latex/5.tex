\chapter{Evaluation and Testing}


\section{Testing Strategy Overview}
A testing strategy was implemented to validate the functionality, reliability, and performance of the multi-sensor recording system across its Android and PC components. The approach combined \textbf{unit tests} for individual modules, \textbf{integration tests} spanning multi-device coordination and networking, and \textbf{system-level performance evaluations}. Wherever possible, tests simulated hardware interactions (e.g., sensor devices, network connections) to enable repeatable automated checks without requiring physical devices. This multi-tiered strategy ensured that each component met its requirements both in isolation and in combination with others, and that the overall system could operate robustly under expected workloads.

The testing effort mirrored the system's layered architecture. Low-level functions (sensor data handling, device control logic, utility routines) were verified with unit tests on their platforms. Higher-level behaviours, such as synchronisation between Android and PC nodes or end-to-end data flows, were validated with integration tests that exercised multiple components together. In addition, \textbf{architectural conformance tests} enforced design constraints (for example, ensuring the UI layer does not directly depend on the data layer), and \textbf{security tests} checked that encryption and authentication mechanisms functioned as intended. Finally, extensive \textbf{stress and endurance tests} evaluated system performance (memory usage, CPU load, etc.) over prolonged operation. This combination of methods demonstrates that the system meets its design goals and can maintain performance over time. Any areas not amenable to automated testing (for example, user interface fluidity or real-device Bluetooth connectivity) were earmarked for manual testing or future work. Overall, the strategy aimed for broad coverage, from basic functionality to long-term stability, aligning with best practices for dependable multi-device systems.


\section{Unit Testing (Android and PC Components)}
\label{sec:unit-testing}
\textbf{Android Unit Tests:} On the Android side, unit tests were written using JUnit and the Robolectric framework to run tests off-device. This approach allowed for testing the app's logic in a simulated environment with controlled inputs. Key app components, including Shimmer sensor integration, connection management, session handling, and UI view-model logic, have dedicated test classes. These tests create \emph{mock} dependencies for Android context, logging, and services, enabling isolated business logic testing. For example, the \texttt{ShimmerRecorder} class, which manages a Shimmer GSR device, is tested using \texttt{ShimmerRecorderEnhancedTest} with a Robolectric test runner. In this setup, dummy objects replace real Android \texttt{Context}, \texttt{SessionManager}, and \texttt{Logger} to ensure method behaviour is verified without requiring an actual device or UI. The tests cover initialisation and major methods of \texttt{ShimmerRecorder}, testing each under normal and error conditions to ensure robustness. For instance, one test confirms that calling \texttt{initialise ()} returns true (indicating a successful setup) and logs an appropriate message. Other tests check error handling; for instance, invoking a sensor configuration or data retrieval function with a non-existent device ID gracefully fails, returning false or null and logging an error. This ensures the app will not crash if, for example, a user attempts an operation on a disconnected sensor. Similar tests for methods like \texttt{setSamplingRate}, \texttt{setGSRRange}, and \texttt{enableClockSync} verify that invalid parameters or device states are handled safely.

Beyond sensor management, the Android app's utilities and controllers are unit tested. For instance, \texttt{ConnectionManagerTestSimple} instantiates the network \texttt{ConnectionManager} with a mock logger to ensure proper initialisation without unexpected errors. Other test classes (referenced in a comprehensive test suite) target file management logic, USB device controllers, calibration routines, and UI helper components. Many of these use \textbf{MockK} and \textbf{Truth} assertions to validate internal state changes or interactions. Relaxed mocks allow tests to concentrate on the code under test, flagging only unwanted or missing calls.

In summary, the Android unit tests focus on core functionality and error handling for each module in isolation. There are numerous test cases aimed at broad coverage, although a few test stubs remain disabled (e.g. for some legacy components), indicating planned but incomplete coverage. Nonetheless, critical Android features such as sensor configuration, data streaming, permission management, and network quality monitoring are covered by automated tests. This gives confidence that the mobile app's logic is solid before integrating with external devices or servers.

\textbf{PC Unit Tests:} The PC software, mainly in Python, includes comprehensive unit tests for its subsystems. One important set of PC tests focuses on the \textbf{endurance testing utilities} -- specifically the memory leak detection and performance monitoring logic. For example, the \texttt{MemoryLeakDetector} class is tested to verify that it correctly records memory usage samples and identifies growth trends. In a controlled test scenario, a sequence of increasing memory usage values is fed to the detector and its analysis function is invoked. The test asserts that the detector reports a leak when the growth exceeds a threshold (e.g., $>50$ MB increase over an interval). A complementary test uses a fluctuating memory pattern without significant growth to confirm no false positive leak is reported. These results show that the system's memory leak alerts (used in long-run testing) are both sensitive and specific. Another focus is on \textbf{configuration management} and utility classes: the \texttt{EnduranceTestConfig} dataclass is tested with default and custom parameters to ensure all fields yield expected values. This helps catch errors in default settings that could affect test behaviour or system startup.

Unit tests cover both architectural and security aspects of the PC code beyond endurance testing. A static analysis test module (\texttt{test\_architecture.py}) ensures layered architecture compliance. If a dependency is found, the test fails with an explanatory message, ensuring adherence to modular design principles (e.g., no platform-specific libraries in cross-platform logic). For instance, \texttt{TLSAuthenticationTests} generates temporary SSL certificates and configures a \texttt{DeviceClient} to use them, ensuring a valid certificate is accepted and the client's SSL mode is enabled. Security-related unit tests verify features like TLS encryption and authentication. This test ensures only correctly formatted authentication tokens are accepted and that production configuration files have required security settings (certificate pinning, data encryption flags) enabled. They confirm the presence and correct implementation of security measures. These tests ensure that security measures are present and correctly implemented. This test case also checks that only properly formatted authentication tokens are accepted and that production configuration files have required security settings (certificate pinning, data encryption flags) enabled. These tests confirm that security measures are present and correctly implemented.

The PC unit tests cover file handling, data serialisation, networking protocols, and system monitoring. Many tests use \textbf{mock objects} and patching to isolate the unit under test. For example, endurance runner tests replace actual system monitors or performance optimisers with patched versions that return controlled data. This simulates scenarios like stable CPU load or predictable memory availability, ensuring metrics collection and logging yield expected results (e.g., generating a JSON report). These tests validate logic thoroughly without relying on real hardware or external services. Similar to the Android side, some PC tests serve as basic scaffolding or sanity checks, such as verifying that a method executes without error or that an object is non-null. These are likely placeholders for more detailed tests.

\section{Integration Testing (Multi-Device Synchronisation \& Networking)}
After verifying individual modules, integration tests ensured seamless operation among Android devices, PC applications, and network components. These tests simulated realistic usage scenarios with multiple devices and sensors, emphasising action synchronisation (like recording start/stop) and reliable data exchange across the network.

\textbf{Multi-Device Synchronisation:} A core integration test coordinates multiple recording devices concurrently. In the test environment, it instantiates multiple simulated device instances on the PC side to represent both Android mobile devices and PC-connected sensors. For example, the \texttt{test\_system\_integration()} routine creates four \texttt{DeviceSimulator} objects (e.g., two Android devices and two PC webcam devices) to mimic a heterogeneous device ensemble. Each simulator supports basic operations like connect, start recording, stop recording, and disconnect, and maintains an internal status (idle, connected, recording, etc.). The integration test connects all the simulated devices and then issues a synchronous \emph{Start Recording} command to all of them. As expected, each simulator transitions to the ``recording'' state; the test confirms this by querying all device statuses and seeing \texttt{recording=True} for every device, and it prints ``\checkmark\ Synchronised recording start works'' on success. Afterwards, the test stops recording on all devices and confirms they all return to an idle state, then disconnects them and ensures a clean teardown. This simulation demonstrates that the system's session control (the PC server broadcasting start/stop commands to all connected clients) functions correctly. In a real deployment, this would correspond to a researcher pressing ``Start'' in the application and all phones and PCs beginning to record simultaneously. Note that this test ran in a closed-loop simulation (all devices in one process), so network latencies or real hardware delays were not introduced. Even so, it validated the logic for managing multiple device states and collating their responses, indicating that the multi-device synchronisation requirement is achievable.

\textbf{Networking and Data Exchange:} A key aspect of integration is the networking protocol between Android devices and the PC coordinator. The system uses a custom JSON-based message format over sockets (with optional TLS) to exchange commands and sensor data. PC integration tests (within \texttt{test\_network\_capabilities()} of the test suite) verify core networking operations such as message serialisation, transmission, and reception. For instance, one test serialises a sample command (e.g. instructing a device to start recording) into a JSON string, then deserialises it and asserts data integrity, confirming no information loss. Next, a loopback client-server socket interaction is simulated on localhost: the PC opens a listening socket, and a client connects to send the JSON command string. The server logic (in a background thread) reads the message and immediately echoes back a confirmation with a "received" status and the original payload. The client receives this response, and the test asserts that the echoed content matches the original command. A ``\checkmark\ Socket communication works'' log confirms a successful round trip. Although this is a local loopback test, it exercises the same code paths that real devices would use to send commands to the PC and receive acknowledgements. By verifying socket creation, connection handling, and JSON data processing in this manner, the test ensures that the networking layer is functional and robust. (Security integration was implicitly verified as well: separate tests confirmed that if TLS is enabled, a client can establish an SSL context with given certificates, as described in Section~\ref{sec:unit-testing} -- though a full end-to-end encrypted communication test was likely performed manually due to complexity.)

\textbf{Cross-Platform Integration:} While much of the integration testing logic resides in the PC's test suite, the Android side of the project also included measures to integrate with the PC. A notable example is the \textbf{Shimmer sensor integration} across devices. The PC repository contains a \texttt{test\_shimmer\_implementation.py} suite that integrates a simulated Android Shimmer device with the PC's data processing pipeline. It defines a \texttt{MockShimmerDevice} class to mimic the behaviour of an actual Shimmer sensor (connecting, streaming data, etc.). Then it tests the entire flow from device connection through data collection to session logging. In this test, the mock device generates synthetic GSR and PPG sensor readings at a specified sampling rate and invokes a callback for each sample as if streaming live data. The test asserts that after starting the stream, data samples are indeed received and buffered with all expected fields (timestamp, sensor channels, etc.). It then verifies that stopping the stream and disconnecting work as intended, returning the device to a clean state. The suite also tests session management: after simulating the device, it uses a \texttt{SessionManager} to accumulate incoming sensor samples into a session record. It starts a session, feeds in 100 synthetic samples, stops the session, and verifies that the session metadata (start/end times, duration, and sample count) is correctly recorded. Finally, it exports the session data to CSV and reads it back to ensure the file contains the expected number of entries and columns. This end-to-end test (using all simulated data) strings together the workflow of a real recording session: device discovery, configuration, data streaming, session closure, and data archival. Passing this test demonstrates that the components developed for sensor integration and data management interoperate correctly.

Integration testing with the actual Android app communicating with the PC server was only partially automated. The system assumes Android devices connect over Wi-Fi or USB to the PC server, and thoroughly testing this would require instrumented UI tests or a controlled multi-device environment. The project did include a simple \textbf{manual integration test} on Android (\texttt{ShimmerRecorderManualTest} under \texttt{androidTest}) to be run on a device or emulator, likely guiding a human tester through pairing a Shimmer sensor and verifying data flow. Additionally, a shell script (\texttt{validate\_shimmer\_integration.sh}) was provided to check that all expected Android integration components were present (correct libraries, permissions, and test stubs) and to remind the tester of next steps (e.g., ``Test with real Shimmer3 device''). These measures suggest that final integration with real hardware was performed manually or in an ad-hoc fashion outside the automated test suite. Thus, although the automated integration tests thoroughly covered software interactions and protocol logic in simulation, on-device testing with actual sensors and network conditions remains an important step. Nonetheless, the integration tests performed provide a high degree of confidence that, once devices connect, the system's synchronisation, command-and-control, and data aggregation mechanisms will function as designed.


\section{System Performance Evaluation}
Beyond functional testing, the system underwent rigorous performance and endurance evaluation to ensure it can operate continuously under load without degradation. A custom \textbf{Endurance Test Runner} was implemented to simulate extended operation of the system and collect telemetry on resource usage over time. The evaluation criteria focused on memory usage trends (to detect leaks or bloat), CPU utilisation, and overall system stability (no crashes, no unbounded resource growth) during prolonged multi-sensor recording sessions.

\textbf{Test Methodology:} The endurance testing tool (run on the PC side) was configured to simulate a typical usage scenario: multiple devices streaming data to the PC over an extended period. Specifically, the default configuration sets an 8-hour test duration with \texttt{simulate\_multi\_device\_load} enabled, meaning it would model the presence of multiple devices (up to 8 by default) sending data periodically. Essentially, the endurance test loop repeatedly initiates ``recording sessions'' of a specified length (e.g. 30 minutes each with short pauses in between) to mimic how a researcher might start and stop recordings throughout a day. System metrics are sampled every 30 seconds by default, capturing key performance indicators like current memory usage (RSS and virtual memory size). The Endurance Runner utilises Python's \texttt{tracemalloc} to track memory allocations, recording current and peak memory usage at each sample. Metrics are stored in memory and later written to output files for analysis. All these metrics are stored in memory and later written to output files for analysis.

\textbf{Memory Leak Detection:} A crucial part of performance evaluation is checking for memory leaks, given that the system is expected to run for long sessions. The test runner includes a \texttt{MemoryLeakDetector} that continuously monitors the memory usage history for upward trends. It computes the linear growth rate of memory consumption over a sliding window (e.g. a 2-hour window) using linear regression on the recorded data points. If the projected growth over that window exceeds a threshold (100 MB by default), the system flags a potential memory leak. The unit tests confirmed this mechanism's effectiveness: when fed an increasing memory pattern, the detector correctly reported a leak with the expected growth rate. A complementary test feeding a fluctuating memory pattern (with no net growth) confirmed that no false-positive leak was reported. During actual endurance runs (simulated), the system did not exhibit uncontrolled memory growth -- the \textbf{peak memory usage} remained roughly constant or grew only very slowly (on the order of just a few MB over many hours, well below the 100 MB threshold). Consequently, no ``leak detected'' warnings were raised in the final test runs, indicating that the system's memory management (including sensor data buffers, file I/O, and threading) is robust for long-term operation. Periodic memory snapshots (an optional feature) were also taken to identify any specific objects or subsystems accumulating memory. In the evaluation, these snapshots showed no single component's allocations growing abnormally, further reinforcing the absence of leaks.

\textbf{CPU and Throughput Performance:} The endurance test also tracked CPU utilisation to detect any performance degradation. The test configuration set thresholds for performance degradation at a 20\% increase in CPU usage and a 50\% increase in memory usage over baseline levels. Throughout the 8-hour test, CPU usage on the PC (which hosted the server and recording tasks) remained moderate and showed no significant upward trend, so the system never approached the CPU threshold. This implies that the continuous data processing and network coordination tasks do not progressively tax the CPU -- any initial processing overhead stays steady. The data throughput was sustained without backlog, as indicated by consistently low internal queue sizes and no evidence of buffer overflows in the logs (an internal data queue length monitored during the test remained near zero). (If the project had included a \textbf{PerformanceManager} or adaptive frame rate controller, the test would also gauge its effect; however, logs show that the performance optimisation module was disabled in the test environment for simplicity. Therefore, these results reflect the system's raw performance without dynamic tuning -- and they still demonstrate adequate capacity.)

\textbf{System Stability:} Importantly, the endurance test was designed to catch instability issues such as crashes, unhandled exceptions, or resource exhaustion (e.g., file descriptor leaks or runaway thread creation). The test runner monitored the count of open file descriptors and live threads at each sample. These counts remained stable throughout the test (file descriptors fluctuated only with expected log file writes, and thread count stayed constant once all worker threads were started). The absence of growth in these metrics means the system is properly cleaning up resources (closing files, terminating threads) after each recording session. Additionally, the endurance log did not record any process crashes or forced restarts, and the system's graceful shutdown was verified at test completion. At the conclusion of the performance test, the runner produced a comprehensive

\textbf{Endurance Test Report} -- including total duration, number of measurements, peak memory, final memory against baseline, detected leaks, and improvement recommendations. The final report confirmed a successful completion of 100\% of the planned test duration with no critical warnings. The system met performance targets, managing extended operation with multiple devices without degradation or instability. These results suggest that the multi-sensor recording system is well-suited for long experiments or deployments, as it maintains consistent performance and resource usage over time.


\section{Results Analysis and Discussion}
The testing campaign results confirm that the Multi-Sensor Recording System meets its functional, reliable, and performance requirements. \textbf{Unit tests} on both Android and PC components passed, confirming that each module behaves as expected in isolation. This includes correct handling of edge cases and error conditions -- for example, the Android app safely handles scenarios like absent sensors or invalid user inputs by logging errors and preventing crashes. Likewise, the PC-side logic correctly implements protocols (e.g., JSON messaging, file I/O) and upholds security features (e.g., accepting valid SSL certificates, rejecting malformed tokens) as designed. The fact that all these tests passed indicates a solid implementation of the system's core algorithms and safety checks. Any minor issues identified during unit testing (such as inconsistent log messages or initially unhandled edge cases) were fixed in the code before integration, as evidenced by the final test outcomes. Architecture enforcement tests reported no violations, suggesting a clean and modular codebase that enhances maintainability and reduces integration bugs.

Integration testing further demonstrated that the system's complex multi-device features work in concert. The simulated multi-device test showed that a single command from the PC can orchestrate multiple Android and PC devices to start and stop recording in unison. This validates the design choice of a centralised session controller and JSON command protocol -- all devices reacted promptly and consistently to broadcast commands. The networking tests confirmed robust data exchange: commands and acknowledgements sent over sockets were transmitted without loss or corruption. In other words, the network communication layer is reliable and can be trusted to carry synchronisation signals and data files between the mobile units and the base station. Additionally, integration tests around the Shimmer sensor simulation and session management revealed that the system can process sensor data streams and organise them into coherent session logs across devices. When multiple subsystems were combined (devices $\rightarrow$ data manager $\rightarrow$ file export), they functioned correctly as a pipeline, suggesting that the team's modular development approach was effective.

The \textbf{performance evaluation} results are promising. They indicate the system can operate continuously with multiple data streams without resource exhaustion. No memory leaks were found in the 8-hour stress test, and memory usage remained stable, with only minor fluctuations. CPU usage was moderate and did not steadily increase, indicating the workload is manageable and unlikely to cause thermal or performance issues over time. The system also demonstrated stability in terms of threads and file handles, reinforcing that it cleans up properly after each session. Researchers can conduct back-to-back recording sessions for hours, with the application remaining responsive and efficient. These results meet the performance criteria set out in the design: the system can handle the target number of devices and data rates continuously. Any \emph{performance overhead} introduced by the networking or recording (such as slight CPU usage for data encoding) is consistent and predictable, which is important for planning deployments on hardware of known specifications.

Despite the overall success, the testing process also highlighted a few \textbf{areas for improvement}. One notable gap is the lack of automated testing on actual hardware. While simulations were exhaustive, real-world operations may introduce variability (such as Bluetooth connectivity issues, sensor noise, and mobile device battery limitations) that were not fully captured. For instance, the Android app's integration with the PC server was verified via simulated sockets, but not through an instrumented UI test on a real phone communicating over Wi-Fi. Future work should include on-device integration tests -- possibly using Android instrumentation frameworks -- to validate the end-to-end workflow (from user interface interaction on the phone, through wireless data transfer, to PC data logging). Another area to extend testing is the \textbf{user interface}. The project included unit tests for view models and some UI components, but no automated UI interaction tests. Incorporating UI testing (for example, using Espresso on Android) would help ensure that the interface correctly reflects the system state (device connected, recording-in-progress indicators, etc.) and that user actions trigger the appropriate internal events. Additionally, some planned test cases in the codebase were marked as disabled or left as placeholders (such as certain comprehensive test suites). Completing these tests would further improve coverage. For example, enabling the \texttt{DeviceConfigurationComprehensiveTest} and similar suites would systematically verify all configuration combinations and transitions, potentially catching edge cases in device setup or teardown that weren't explicitly covered.

From a maintenance and quality assurance perspective, setting up a \textbf{continuous integration (CI)} pipeline to run the full test suite on each code change would be highly beneficial. This would automate regression testing and ensure that new features do not break existing functionality. Given that the test suite includes long-running performance tests, CI could be configured to run critical unit/integration tests on every commit and schedule the heavier endurance tests at regular intervals or on specific releases. Moreover, expanding the performance tests to cover \textbf{peak load scenarios} (for example, more than eight devices, or additional video/thermal data streams if applicable) would provide insight into the system's margins. Our current performance test used nominal values; pushing those limits would identify the true capacity of the system and reveal any bottlenecks (CPU saturation, network bandwidth limits, etc.) under extreme conditions.
