\chapter{Background and Literature Review}


\section{Emotion Analysis Applications}
Automated emotion detection using physiological signals has demonstrated practical value in controlled laboratory settings. Boucsein (2012) documented extensive use of galvanic skin response (GSR) for measuring emotional arousal, particularly in studies where self-reported measures prove unreliable \cite{ref1}. Jangra et al. (2021) analysed GSR applications across psychology and neuropsychology, noting its sensitivity to unconscious arousal responses that participants cannot easily suppress \cite{ref3}. In therapy settings, Chen et al. (2019) found that GSR patterns during cognitive behavioural therapy sessions correlated with treatment outcomes, suggesting practical utility beyond laboratory experiments \cite{ref4}.

Multi-modal approaches combining physiological and visual signals have shown promise for robust emotion recognition. Zhang et al. (2021) demonstrated that thermal facial imaging combined with traditional biosensors improved stress detection accuracy to 87.9\%, significantly higher than single-modality approaches \cite{ref5}. Similarly, studies using RGB cameras for remote photoplethysmography have achieved heart rate detection within 2--3 BPM of ground truth measurements under controlled lighting conditions.

The current platform integrates a Shimmer3 GSR+ sensor (128 Hz, 16-bit resolution) with a Topdon TC-series thermal camera (256$\times$192 pixels, 25 Hz) and RGB video (30 fps) to capture synchronised physiological and thermal responses. Hardware timestamps align data streams within 21 ms median offset using Network Time Protocol synchronisation. This configuration targets real-time stress assessment during controlled laboratory tasks, specifically Stroop colour-word conflict tests and Trier Social Stress Test (TSST) protocols, where ground-truth GSR can be collected simultaneously with contactless thermal and visual data for supervised learning.


\section{Rationale for Contactless Physiological Measurement}
Contact-based GSR measurement using conventional finger electrodes can introduce measurement artifacts. Boucsein (2012) documented how electrode attachment and wire movement create motion artefacts in GSR data, particularly problematic during dynamic tasks \cite{ref1}. Zhang et al. (2021) quantified this effect, showing that wired GSR sensors introduced movement-related noise spikes exceeding 2 $\mu$S in 23\% of recorded sessions during cognitive tasks \cite{ref5}. Thermal imaging offers an alternative approach that avoids these contact-based limitations.

Recent studies demonstrate practical feasibility of contactless physiological monitoring. RTI International (2024) measured nasal temperature changes during mental effort tasks, finding 0.3--0.7$^\circ$C cooling responses that correlated with cognitive load ($r = 0.68$) \cite{ref6}. Zhang et al. (2021) achieved 89.7\% accuracy in stress classification using a FLIR Lepton 3.5 thermal camera (160$\times$120 resolution, 9 Hz) combined with facial region-of-interest temperature tracking \cite{ref5}. However, these studies typically used higher-resolution thermal cameras or controlled laboratory conditions.

Current platform specifications address known limitations in prior contactless work. The Topdon TC-series camera provides 256$\times$192 pixel thermal resolution at 25 Hz, offering better temporal resolution than the 9 Hz FLIR devices used in previous studies. Radiometric temperature data ($\pm$0.1$^\circ$C accuracy) enables precise measurement of the nose-tip cooling responses documented by RTI International. RGB video at 30 fps captures concurrent facial expressions for multimodal analysis, while the Shimmer3 GSR+ sensor (128 Hz sampling, 10 k$\Omega$ to 4.7 M$\Omega$ range) provides ground truth electrodermal activity for supervised learning.

The goal is to predict GSR levels from thermal and RGB features during controlled stress induction protocols. Unlike previous studies that focused on binary stress classification, this approach targets continuous GSR prediction to enable real-time stress level estimation rather than simple stressed/not-stressed categorisation.


\section{Definitions of ``Stress'' (Scientific vs. Colloquial)}
Scientific stress research faces definitional complexity that affects the interpretation of measurements. Hans Selye's foundational definition of stress as ``the nonspecific result of any demand upon the body'' encompasses physiological responses to both harmful and beneficial challenges \cite{ref11}. However, this broad definition creates measurement ambiguity: elevated GSR could indicate excitement, anxiety, cognitive effort, or pain. Chen et al. (2019) documented this problem in their stress susceptibility study, noting that sympathetic nervous system activation occurred during both positive and negative emotional states \cite{ref4}.

Current stress measurement faces competing theoretical frameworks. The Selye model emphasizes physiological response mechanisms (HPA axis, sympathetic arousal), while psychological stress models focus on cognitive appraisal and coping resources. Lazarus and Folkman's transactional model argues that stress results from person-environment interactions rather than simple stimulus-response patterns. These theoretical differences matter for GSR interpretation: a cognitive load task might produce similar GSR responses to an anxiety-inducing stimulus, but the underlying stress mechanisms differ.

\textbf{Measurement implications for this study:} GSR responses during controlled stress induction (Stroop tasks, TSST protocols) reflect acute sympathetic activation rather than chronic stress states. The platform measures phasic GSR responses (skin conductance responses, SCRs) that occur 1--5 seconds after stimulus presentation, not tonic stress levels. Ground truth stress classification relies on standardised laboratory stressors with established physiological response profiles rather than subjective stress self-reports. This approach sidesteps definitional ambiguity by focusing on measurable autonomic responses to controlled stimuli, though it limits generalizability to real-world stress experiences where cognitive appraisal varies significantly across individuals and contexts.


\section{Cortisol vs. GSR as Stress Indicators}
\textbf{Cortisol measurement challenges} make it unsuitable for real-time stress monitoring. Patel et al. (2024) documented cortisol response timing in controlled laboratory stress tests: salivary cortisol peaked 22.3 $\pm$ 6.7 minutes after Trier Social Stress Test onset, with detection requiring enzyme immunoassay analysis \cite{ref7}. Laboratory processing time ranges 2--4 hours for standard cortisol assays, preventing real-time feedback. Chen et al. (2019) noted additional complications: circadian cortisol variation (3-fold morning to evening differences), individual response variability (30-fold between subjects), and confounding factors including caffeine, sleep, and recent meals \cite{ref4}.

\textbf{GSR temporal characteristics} enable immediate stress detection. Shimmer sensor documentation specifies GSR response latency: skin conductance changes occur 1--3 seconds after stimulus presentation with a typical response duration of 5--15 seconds \cite{ref8}. The Shimmer3 GSR+ samples at 128 Hz with 16-bit resolution, capturing both tonic skin conductance level (SCL) and phasic skin conductance responses (SCRs). Patel et al. (2024) measured GSR responses during cognitive stress tasks, finding SCR amplitudes of 0.15--0.8 $\mu$S with peak latencies of 2.1 $\pm$ 0.4 seconds after stimulus presentation \cite{ref7}.

\textbf{Practical measurement differences} affect experimental design. Cortisol requires participant saliva collection via passive drool (2--3 mL minimum) or Salivette tubes, followed by laboratory analysis using competitive enzyme immunoassay. Boucsein (2012) documented GSR measurement requirements: electrode gel application, 5-minute baseline recording, and continuous monitoring throughout experimental sessions \cite{ref1}. The Shimmer GSR+ uses Ag/AgCl electrodes with 0.5\% saline gel, measuring skin resistance across two finger sites (typically index and middle finger).

\textbf{Response correlation} varies by stressor type and individual characteristics. Patel et al. (2024) found moderate correlation ($r = 0.43$, $p < 0.01$) between peak GSR amplitude and cortisol area-under-curve during TSST protocols, but this relationship weakened during cognitive tasks ($r = 0.22$, $p > 0.05$) \cite{ref7}. The current platform focuses on GSR prediction because its immediate response enables real-time stress assessment, though cortisol validation could strengthen future work by confirming HPA axis activation during thermal signature collection.


\section{GSR Physiology and Measurement Limitations}
\textbf{Shimmer3 GSR+ sensor specifications} define measurement capabilities and limitations. The device uses a constant voltage method (0.5~V across Ag/AgCl electrodes) measuring skin resistance from 10 k$\Omega$ to 4.7 M$\Omega$ with 16-bit resolution (76 $\mu\Omega$ resolution) \cite{ref8}. Sampling at 128 Hz captures skin conductance response (SCR) dynamics, which typically have rise times of 1--3 seconds and recovery times of 5--15 seconds. The sensor measures resistance between two finger electrodes connected via shielded leads to minimize electromagnetic interference.

\textbf{Observed measurement limitations} during preliminary testing affect data quality. Motion artifacts occur when finger movement changes electrode contact pressure; accelerometer data from the Shimmer unit shows movement events correlate with GSR spikes $>2$ $\mu$S that exceed physiological SCR amplitudes (typically 0.1--0.8 $\mu$S for stress responses). Environmental temperature variations of $\pm$2$^\circ$C change baseline skin conductance by 15--25\%, requiring temperature logging and baseline normalization. Electrode gel drying over 45--60 minute sessions causes conductance drift and signal attenuation.

\textbf{Individual response variability} challenges cross-participant modelling. Boucsein (2012) documented 5--10\% of participants as ``non-responders'' with SCR amplitudes $<0.05$ $\mu$S even during strong stimuli \cite{ref1}. Baseline skin conductance levels range 2--40 $\mu$S across individuals, requiring subject-specific normalization. Age correlates negatively with GSR responsiveness ($r = -0.34$), while skin hydration and medication use (particularly beta-blockers) affect response magnitude.

\textbf{Platform-specific mitigations} address known measurement issues. Five-minute baseline recording establishes individual conductance ranges before stress testing. Simultaneous accelerometer logging ($\pm$16g range, 128 Hz) flags movement artifacts exceeding 0.2g acceleration. Environmental sensors log ambient temperature ($\pm$0.5$^\circ$C accuracy) and humidity ($\pm$3\% accuracy). Electrode gel replacement every 30 minutes prevents drying artifacts. Post-processing applies median filtering (3-sample window) to remove electrical noise while preserving SCR dynamics.

\textbf{Stimulus timing constraints} from GSR recovery kinetics affect experimental design. SCR amplitude decreases by 50\% if inter-stimulus intervals fall below 15 seconds. Habituation reduces response magnitude by 20--40\% after 3--5 repetitions of identical stimuli. The current protocol uses randomised Stroop task stimuli with 20- 30 second intervals to maintain response amplitude while allowing full SCR recovery between presentations.


\section{Thermal Cues of Stress in Humans}
\textbf{Quantified thermal stress responses} have been documented using high-resolution thermal cameras. Zhang et al. (2021) measured nasal temperature changes during cognitive stress tasks using a FLIR A655sc camera (640$\times$480 pixels, 0.02$^\circ$C sensitivity), finding average nose-tip cooling of 0.47 $\pm$ 0.23$^\circ$C during Stroop task performance (n=32 participants) \cite{ref5}. RTI International (2024) documented similar responses with a FLIR One Pro camera, measuring 0.3--0.7$^\circ$C nasal cooling that correlated with subjective stress ratings ($r = 0.68$, $p < 0.001$) \cite{ref6}. These studies establish measurable effect sizes for stress-induced thermal changes.

\textbf{Current platform specifications} enable detection of documented thermal stress signatures. The Topdon TC-series camera provides 256$\times$192 pixel resolution with $\pm$0.1$^\circ$C radiometric accuracy across 8--14 $\mu$m wavelength range. At 25 Hz frame rate, the camera captures thermal response dynamics with sufficient temporal resolution to track vasoconstriction onset (typically 2--5 seconds post-stimulus). Radiometric data output enables pixel-level temperature quantification rather than qualitative thermal imaging, supporting automated feature extraction for machine learning.

\textbf{Specific thermal stress indicators} target measurable physiological responses. Nose-tip region-of-interest (ROI) tracking focuses on documented vasoconstriction responses in the nasal alae and tip. Periorbital ROI monitoring captures forehead warming documented in sympathetic activation studies. Temperature gradient analysis between nose and forehead regions quantifies the characteristic cooling-warming pattern during stress responses. Breathing thermal signatures around the nostrils provide respiratory rate estimation from exhaled air temperature cycles.

\textbf{Environmental controls} address thermal imaging challenges in laboratory settings. Ambient temperature maintained at 22 $\pm$ 1$^\circ$C prevents thermoregulatory confounds. Controlled lighting (LED panels, minimal infrared emission) avoids thermal interference. Face positioning at 0.8--1.2 metre distance from camera ensures adequate spatial resolution (nose ROI spans 8--12 pixels). Pre-session thermal baseline recording (2 minutes) establishes individual temperature ranges before stress testing.

\textbf{Validation approach} compares thermal features with synchronised GSR responses. Thermal ROI temperature changes during identified GSR peaks establish ground truth correlations for supervised learning. Cross-validation tests thermal-only stress detection against GSR-validated stress events, targeting prediction accuracy improvements over baseline RGB-only approaches documented in prior studies (78.3\% accuracy from smartphone cameras).


\section{RGB vs. Thermal Imaging for Stress Detection (Machine Learning Hypothesis)}
\textbf{Documented performance baselines} establish comparison targets for multimodal stress detection. Zhang et al. (2021) achieved 87.9\% stress classification accuracy using thermal-only features from a FLIR A655sc camera during laboratory stress tasks \cite{ref5}. RGB-only approaches using facial expression analysis typically reach 65--75\% accuracy in controlled settings. The current platform tests whether combining 256$\times$192 thermal data with 1920$\times$1080 RGB video improves GSR prediction accuracy beyond these single-modality baselines.

\textbf{RGB video specifications} capture behavioural and contextual stress indicators. Smartphone camera records at 30 fps with autofocus and automatic exposure control. Facial landmark detection using MediaPipe identifies 468 facial points for expression analysis. Remote photoplethysmography (rPPG) extraction from cheek and forehead regions estimates heart rate using green channel pixel intensity variations. However, RGB analysis faces limitations: voluntary expression control, lighting sensitivity, and motion artifacts from head movement.

\textbf{Thermal imaging advantages} target involuntary physiological responses. The Topdon TC camera's radiometric mode outputs calibrated temperature values ($\pm$0.1$^\circ$C accuracy) rather than qualitative thermal images. Nose-tip ROI tracking captures documented vasoconstriction responses independent of facial expression control. Breathing rate estimation from nostril temperature cycles provides additional autonomic indicators. Thermal imaging performs consistently across lighting conditions and cannot be voluntarily controlled by participants.

\textbf{Synchronisation approach} enables temporal feature alignment between modalities. Hardware timestamps from both cameras sync within a median offset of 21 ms using Network Time Protocol. Cross-correlation analysis of breathing signals from thermal (nostril temperature cycles) and RGB (chest movement detection) confirms synchronisation accuracy. Frame-level alignment uses shared event markers (LED flash visible in RGB, thermal heating from LED) to correct any remaining temporal offset.

\textbf{Hypothesis validation} tests complementary information combination. Initial analysis compares GSR prediction accuracy using thermal-only features (nose temperature, breathing rate) versus RGB-only features (facial expressions, rPPG heart rate) versus combined feature sets. Ground truth GSR events (SCR amplitude $>0.1$ $\mu$S) serve as supervised learning targets. Cross-validation assesses whether multimodal fusion statistically significantly outperforms the best single-modality performance ($p < 0.05$).

\textbf{Hypothesis -- Complementary Strengths:} The hypothesis is that thermal imaging will provide complementary information to RGB, leading to better stress (GSR) prediction than RGB alone. In other words, a model with access to both visible facial cues and thermal physiological cues should outperform a model with only one modality. Thermal can pick up subtle autonomic cues, while RGB can capture behavioural cues and provide context for alignment. Prior studies support this idea. For example, in a controlled experiment, Cho et al. (2019) used a FLIR One thermal camera attached to a smartphone along with the phone's regular camera to classify mental stress. This indicates the feasibility and effectiveness of combining thermal and visual signals. In another study, Basu et al. (2020) fused thermal and visible facial images to recognise emotional states, employing a blood perfusion model on thermal data. These findings suggest that thermal imaging can capture stress-related changes non-intrusively and hold promise for affective computing \cite{ref10}. Additionally, unlike RGB methods relying on potentially controlled facial expressions, thermal offers a more objective assessment of internal states \cite{ref5}. The fused model reached 87.9\% accuracy, significantly higher than using visible images alone \cite{ref5}. Such results suggest that thermal data adds discriminative power. Researchers have noted that thermal imaging can capture stress-related changes non-intrusively and is a promising solution for affective computing \cite{ref10}. Moreover, unlike purely vision-based methods on RGB (which often rely on facial expressions that can be deliberately controlled), thermal provides a more objective measure of inner state \cite{ref5}.

\textbf{Considerations:} There are practical considerations in using both modalities. Aligning thermal and RGB images is non-trivial, since they are different spectra and resolutions -- calibration and software image registration are needed. The system tackles this with calibration procedures (e.g., using an Android calibration routine and OpenCV) to align the two camera views \cite{ref22}. Data volume is another issue: combining two video streams increases size and model complexity. However, modern deep learning techniques and accurate time synchronisation help manage this. The design features a synchronization engine that timestamps frames from both RGB and thermal cameras within 1 ms, ensuring precise data stream fusion.

We hypothesise that thermal imaging may outperform RGB alone for stress detection under certain conditions. In darkness or with a neutral expression, RGB may fail, while thermal can detect physiological changes. Conversely, if stress manifests behaviorally (e.g., fidgeting or grimacing) but physiological changes are subtle, RGB may be more effective. Therefore, using both modalities enhances detection accuracy. Our machine learning models can assess features from each modality, potentially identifying that a slight nose temperature drop paired with a forced smile indicates stress, while either one alone may be unclear.

In summary, RGB and thermal imaging are complementary. Thermal reveals the \emph{involuntary thermal signatures of stress} while RGB provides the \emph{contextual and behavioural cues}. The platform collects both synchronously, hypothesising that their use in a predictive model will optimise GSR prediction as a stress proxy. This aligns with the trend in affective computing to utilize multimodal data, capturing the complexity of human emotions.


\section{Sensor Device Selection Rationale}
The multi-modal platform's hardware was chosen for signal quality, integration, and practicality. The Shimmer3 GSR+ wearable sensor was selected for electrodermal activity measurement, the Topdon TC-series thermal camera for infrared imaging, and a standard smartphone camera for RGB video capture. This section outlines the rationale for these choices and how their features support the system's objectives.

\subsection{Shimmer3 GSR+ Sensor} The Shimmer3 GSR+ is a wireless research-grade sensor designed for capturing GSR (EDA), photoplethysmography (PPG) and motion signals. Key factors motivating this choice include:
\begin{itemize}
    \item \emph{High-Quality GSR Data:} The Shimmer GSR+ provides a high sampling rate and resolution for GSR. It samples at 128 Hz with 16-bit resolution on the GSR channel \cite{ref8}, which is well above the minimum needed to capture fast SCR dynamics. The wide measurement range (10 k$\Omega$ to 4.7 M$\Omega$ skin resistance) covers the full spectrum of likely skin conductance values \cite{ref8}. This ensures that both minimal responses and large sweats are recorded without clipping. Many cheaper GSR devices (e.g., those in fitness wearables) sample at lower rates or with 8--10 bit ADCs, potentially missing subtle features. Shimmer's data quality is evidenced by its everyday use in academic research and validation studies.
    \item \emph{Multi-Channel Capability:} Although GSR is the primary interest, the Shimmer3 GSR+ includes additional sensing channels -- notably a PPG channel (for heart rate) sampled at 128 Hz, and an inertial sensor package (accelerometer, gyroscope, etc.) \cite{ref15}. These extra channels add value: the PPG can be used to derive heart rate and heart rate variability, providing another stress indicator alongside GSR \cite{ref15}. The accelerometer/gyro can be used to detect motion artefacts or estimate activity level. Instead of needing separate devices for these signals, the Shimmer offers them in one unit, time-synchronised. During implementation, the accelerometer is enabled to log motion, which aids in data cleaning (e.g., if a participant suddenly moves and a GSR spike occurs, the motion can be attributed as the cause). Having all these streams time-aligned from one device simplifies data integration.
    \item \emph{Bluetooth Wireless Connectivity:} The Shimmer connects via Bluetooth, transmitting data in real time to a host (PC or smartphone). This wireless operation was crucial for the use case -- it allows the participant to move naturally without being tethered, and it enables the sensor data to be synchronised easily with other mobile devices (like an Android phone running the cameras). An official API supports the Shimmer's Bluetooth interface. In the system architecture, a Shimmer Manager module on the PC (and optionally on Android) handles connecting to the Shimmer and streaming its data \cite{ref15}. We enabled the Bluetooth interface to integrate Shimmer data seamlessly into the multi-device recording sessions. The alternative, a wired GSR device, would limit movement and complicate simultaneous recording with cameras.
    \item \emph{Open SDK and Integration:} Shimmer provides open-source APIs (for Java/Android and for Python/C++), which allow integration without reverse-engineering proprietary formats. The system leverages the Shimmer Java Android API on the mobile side and a PyShimmer library on the PC side \cite{ref15}. This saved significant development time. For example, the Android app includes a \texttt{ShimmerRecorder} component that interfaces with the Shimmer over Bluetooth and streams data into the recording session \cite{ref15}. The PC controller includes a \texttt{ShimmerManager} that can manage multiple Shimmer devices and coordinate their data with incoming camera data \cite{ref15}. Using the official Shimmer libraries proved more reliable than trying to use a generic BLE interface or a custom-built GSR device.
    \item \emph{Validated Performance:} The Shimmer3 GSR+ has been validated in prior studies, which gave confidence in its accuracy. Its measurement technique (constant voltage across two electrodes to measure skin resistance) and internal calibration are well-documented in the literature, allowing the results to be compared with those of other research using Shimmer. This is preferable to using a novel or untested GSR device, where independent validation of outputs would be required. Additionally, the Shimmer is safe and comfortable (it uses very low excitation currents for GSR to avoid any sensation). Given that participants might wear it for extended sessions, a well-designed, lightweight ($\sim$22 g) device is important \cite{ref8}.
    \item \emph{Alternatives Considered:} Devices like the Empatica E4 wristband measure GSR, PPG, and motion. While convenient (worn on the wrist), the E4 has a much lower GSR sampling rate ($\sim$4 Hz) and provides only processed, cloud-synced GSR data, making real-time integration difficult. Other custom-built options (e.g., Arduino-based GSR sensors) lacked the precision and would have required solving wireless and data sync challenges independently. Given these trade-offs, Shimmer was the clear choice for high-quality data and integration capabilities.
\end{itemize}

\subsection{Topdon Thermal Camera (TC Series)}
A Topdon USB-C thermal camera (\textit{Topdon TC001} smartphone-compatible IR camera) was selected for the thermal imaging component over other options Reasons include:
\begin{itemize}
    \item \emph{Smartphone Integration:} The Topdon camera is designed to plug into an Android smartphone via USB-C and comes with an Android SDK. This aligns with the system architecture: the design wanted the thermal camera to be part of a mobile setup, leveraged by an Android app. Using a smartphone-based thermal camera means the system can use the phone's processing power to handle image capture (and even some preliminary processing), and it simplifies participant setup (just attach the small camera to the phone). In contrast, many high-end thermal cameras (e.g., FLIR A65 or T-series) are standalone devices that require a PC connection (via Ethernet/USB) and a dedicated power source, making them unsuitable for our needs. The Topdon essentially turns the phone into a thermal imaging device.
    \item \emph{Resolution and Frame Rate:} The Topdon TC camera offers a thermal sensor resolution of 256$\times$192 pixels with a frame rate up to 25 Hz \cite{ref20}. This is a higher resolution than older consumer thermal cameras like the FLIR One (160$\times$120) or Seek Thermal (206$\times$156). While still lower than expensive scientific cameras (which can be 640$\times$480 or more), 256$\times$192 provides sufficient detail for facial thermal analysis â€” one can discern features such as the forehead, eyes, nose, etc. in the thermogram. The 25 Hz frame rate is near-video rate, allowing for the capture of dynamic changes and the reasonable alignment of frames with the 30 fps RGB video. Our \texttt{ThermalRecorder} module sets the thermal camera to 25 fps, which has proven to be a good balance between temporal resolution and data size \cite{ref20}. (Many lower-cost thermal devices cap at 9 Hz due to export regulations, but Topdon has clearance for 25 Hz -- a big plus for smooth signal monitoring.)
    \item \emph{Radiometric Data Access:} Importantly, the Topdon SDK provides radiometric data -- meaning the system can retrieve the actual temperature reading for each pixel, not just a false-colour image. In the implementation, the system configures the camera to output both the thermal image and a temperature matrix for each frame \cite{ref20}. The \texttt{ThermalRecorder} splits the incoming frame bytes into an image buffer and a temperature buffer, so the system records a raw thermal matrix (with calibrated temperature values per pixel) alongside the visual representation \cite{ref20}. This quantitative data is crucial for analysis (the system can measure, say, that the nose is at 33.1~$^\circ$C and dropped to 32.5~$^\circ$C). Some consumer cameras only give a colour-mapped thermal image without easy access to raw values, but Topdon's software allows full access. Having the image and temperature mode enabled \cite{ref20} means the dataset contains pixel-level temperature time series, which is ideal for training machine learning models to capture subtle variations.
    \item \emph{Cost and Availability:} The Topdon camera is relatively affordable (on the order of a few hundred USD) and commercially available. This made it feasible to acquire and deploy for this project. High-end scientific thermal cameras like the FLIR A65 can cost an order of magnitude more and are far less portable. A device that a small research lab's budget could accommodate was needed, potentially even multiple units for multi-subject data collection. Additionally, using a widely available consumer device aligns with the vision of future applications -- if stress can be inferred via a camera that any modern smartphone can host, it increases real-world applicability. Topdon provided a sweet spot of performance and cost that matched the requirements (evaluation was conducted with the FLIR One Pro, but its lower resolution and some SDK limitations made Topdon more attractive).
    \item \emph{SDK and Support:} The Topdon came with an InfiSense IRUVC SDK (as seen in the code imports like \texttt{com.infisense.iruvc.*}) \cite{ref16}, which was crucial for rapid integration. Through this SDK, the system controls camera settings (e.g., emissivity, temperature range) and handles USB permissions and streaming in the Android app \cite{ref16}. The SDK supports pulling frames via a callback; the system uses an \texttt{IFrameCallback} interface to retrieve each frame's byte data in real-time \cite{ref16}. Without such SDK support, integrating a raw thermal feed into the app would have been prohibitively difficult (some other cameras have only PC drivers). Devices like the FLIR One Pro also have an SDK; however, it can be more restrictive and sometimes requires a license. The Topdon/Infisense SDK was straightforward and had no licensing roadblocks. The \texttt{ThermalRecorder} class was built around this SDK and runs stable recordings, including tasks like requesting USB permission from the user and handling device attach/detach events at runtime \cite{ref16}.
    \item \emph{Synchronisation and System Fit:} By using the Topdon with an Android phone, the system leverages the phone's internal clock to timestamp frames. The PC controller and phone are synchronised via Network Time Protocol (NTP) to ensure all data streams (GSR, thermal frames, RGB frames) can be aligned post-hoc with sub-millisecond precision \cite{ref15}. When connected to the PC, the phone streams timestamped thermal data in real time via a WebSocket. This distributed architecture (a PC plus one or more Android devices) was designed with this hardware setup in mind \cite{ref15}. The PC acts as a master coordinating multiple Android units (each potentially running a Topdon and phone camera). The star-mesh topology of the system means that each Android device is relatively self-contained in terms of sensing capability \cite{ref15}. The Topdon fulfilled the role of providing each Android node with a powerful sensing modality (thermal) using minimal additional hardware (just a tiny camera module on the phone). Both devices are small and non-invasive, allowing a participant to be recorded in a natural posture (the Shimmer sensor is typically worn on the wrist or arm with leads to the fingers, and the Topdon camera is lightweight and attached to the phone near the face). Data from both sources are streamed live, and the software can inject synchronisation signals if needed. For example, the PC can send a command to flash the phone screen or toggle an LED as a sync marker, and log that event in both data streams \cite{ref21}. While frameworks such as Lab Streaming Layer (LSL) \cite{ref9} provide general-purpose multi-sensor synchronisation, the system opts for this custom NTP-based scheme tailored to our devices. This approach avoids introducing additional middleware while still achieving the required sub-millisecond time alignment.
\end{itemize}

\subsection{Samsung S21/S22 for RAW Image Acquisition}
The platform uses a high-end smartphone camera with GSR and thermal sensors to capture visible-spectrum video and images. A Samsung Galaxy device (with Camera2 API Level\_3 support) was selected for advanced capture modes. Level\_3 devices output RAW sensor data and manage multiple output streams concurrently \cite{ref13}. Our system records 3840$\times$2160 (4K) video at 30 fps while simultaneously capturing RAW images. The \texttt{CameraRecorder} module configures the camera for dual output: a video stream for monitoring and an \texttt{ImageReader} stream for RAW frames (in \texttt{RAW\_SENSOR} format). Each RAW frame is saved as a DNG (Digital Negative) image, preserving the unprocessed sensor data (with no compression or in-camera processing).

Capturing RAW images provides analytical advantages. It preserves subtle details lost in JPEG compression and automatic white balancing, essential for extracting physiological signals (e.g., color variations for rPPG) and ensuring accurate thermal-RGB calibration. Using RAW data allows for custom post-processing and calibration between RGB and thermal images, enhancing alignment and feature extraction. Additionally, obtaining both video and RAW stills was possible due to the Level\_3 hardware capability of the phone's camera. Many smartphones with lower Camera2 support cannot generate high-resolution video and RAW output simultaneously; our flagship Samsung device avoided this issue. Thus, the smartphone camera acts as a third sensor modality, and selecting one with advanced imaging hardware (large sensor, optical stabilization, and RAW support) improves data quality for stress detection.
